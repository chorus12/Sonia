{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "10dc6168c78bffe190597d23394ff54e698792d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "h, w = 64, 64\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "8db07f3dfad330731c1e340995dad80abe7d4293"
   },
   "outputs": [],
   "source": [
    "def resize (img):\n",
    "    im = Image.fromarray(img)\n",
    "    im.thumbnail((h, w), Image.ANTIALIAS)\n",
    "    im = np.array(im).astype('float32')\n",
    "    return np.pad(im, (((h - im.shape[0]) // 2, (h - im.shape[0] + 1) // 2), ((w - im.shape[1]) // 2, (w - im.shape[1] + 1) // 2)), 'constant', constant_values=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d70fc17f961c7b79542dca0400e8d9b0d4aef225"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 9989/332987 [00:02<01:12, 4427.06it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a0f34e785ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a0f34e785ec1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-220aabea652b>\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthumbnail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANTIALIAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepend_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'edge'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_append_const\u001b[0;34m(arr, pad_amt, val, axis)\u001b[0m\n\u001b[1;32m    159\u001b[0m     padshape = tuple(x if i != axis else pad_amt\n\u001b[1;32m    160\u001b[0m                      for (i, x) in enumerate(arr.shape))\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_do_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_do_append\u001b[0;34m(arr, pad_chunk, axis)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_do_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     return np.concatenate(\n\u001b[0;32m--> 103\u001b[0;31m         (arr, pad_chunk.astype(arr.dtype, copy=False)), axis=axis)\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data1 = np.load('train-1.npy')\n",
    "data2 = np.load('train-2.npy')\n",
    "data3 = np.load('train-3.npy')\n",
    "data4 = np.load('train-4.npy')\n",
    "train = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "\n",
    "train[:, 0] = [resize(img) for img in tqdm(train[:, 0])]\n",
    "\n",
    "del data1\n",
    "del data2\n",
    "del data3 \n",
    "del data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c528ffdc5ae1d5d1340ba5cd70764812d1c062e1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 20))\n",
    "for n, (image, tag) in enumerate(train, 1):\n",
    "    if n > 64:\n",
    "        break\n",
    "    plt.subplot(8, 8, n)\n",
    "    plt.title(tag)\n",
    "    plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bf009b1d56cd027683ba41617bb5b2334bd9f69"
   },
   "outputs": [],
   "source": [
    "print(train[10, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c2dff08b62560cb19009d44423295048046e8143"
   },
   "outputs": [],
   "source": [
    "dct = dict(zip(range(1000), np.unique(train[:, 1])))\n",
    "dct = {v:k for k,v in dct.items()}\n",
    "\n",
    "train[:, 1] = [dct[code] for code in train[:, 1]]\n",
    "\n",
    "#train, test = train_test_split(train, test_size=0.2, random_state=113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2bd7c9c37920e0dd2bb5dbbe562bbb167cf43031"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332987/332987 [00:00<00:00, 1383371.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x_np, train_y_np = np.array([i[:, :, np.newaxis] for i in tqdm(train[:, 0])]), np.array(train[:, 1], dtype='int32')\n",
    "del train\n",
    "#test_x_np, test_y_np = np.array([i[:, :, np.newaxis] for i in tqdm(test[:, 0])]), np.array(test[:, 1], dtype='int32')\n",
    "#del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4f9a4f32edb38a2ee2833ad53b841638e05c6955"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "feabf686c3d2d23205c502d83c07133f34577f07"
   },
   "outputs": [],
   "source": [
    "class Hieroglyph_data(Dataset):\n",
    "    def __init__(self, tX, tY = None,\n",
    "                 transform = transforms.Compose(\n",
    "                     [transforms.ToPILImage(), \n",
    "                      transforms.ToTensor(), \n",
    "                      transforms.Normalize(mean=(0.5,), std=(0.5,))]), train=True):\n",
    "        \n",
    "        if train == True:\n",
    "            self.X = tX\n",
    "            self.y = torch.Tensor(tY).type(torch.LongTensor)\n",
    "            self.transform = transform\n",
    "            self.train = True\n",
    "            \n",
    "        else:\n",
    "            self.X = tX\n",
    "            self.y = None\n",
    "            self.transform = transform\n",
    "            self.train = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train == True:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5aaf43cd790df4031e570e6c7ee64dd78e67370a"
   },
   "outputs": [],
   "source": [
    "train_dataset = Hieroglyph_data(train_x_np, train_y_np, transform = transforms.Compose(\n",
    "                                [transforms.ToPILImage(), \n",
    "                                 transforms.RandomAffine(\n",
    "                                 degrees=(-10, 10), \n",
    "                                 translate=(.1, .1), \n",
    "                                 scale=(.9, 1.1),\n",
    "                                 shear=(-10, 10)),\n",
    "                                 transforms.ToTensor(), \n",
    "                                 transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "\n",
    "#test_dataset = Hieroglyph_data(test_x_np, test_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "664d254b99b3232b7fb54ca1d3a6da39781c2d8c"
   },
   "outputs": [],
   "source": [
    "num_classes = 1000\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "af8b7392a2eebc0722b360ffe4888d8495dba1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 30, 30]           1,600\n",
      "       BatchNorm2d-2           [-1, 64, 30, 30]             128\n",
      "            Conv2d-3           [-1, 96, 30, 30]           6,144\n",
      "       BatchNorm2d-4           [-1, 96, 30, 30]             192\n",
      "            Conv2d-5           [-1, 96, 30, 30]           2,592\n",
      "       BatchNorm2d-6           [-1, 96, 30, 30]             192\n",
      "            Conv2d-7          [-1, 272, 30, 30]          26,112\n",
      "       BatchNorm2d-8          [-1, 272, 30, 30]             544\n",
      "            Conv2d-9          [-1, 272, 30, 30]          17,408\n",
      "      BatchNorm2d-10          [-1, 272, 30, 30]             544\n",
      "       Bottleneck-11          [-1, 288, 30, 30]               0\n",
      "           Conv2d-12           [-1, 96, 30, 30]          27,648\n",
      "      BatchNorm2d-13           [-1, 96, 30, 30]             192\n",
      "           Conv2d-14           [-1, 96, 30, 30]           2,592\n",
      "      BatchNorm2d-15           [-1, 96, 30, 30]             192\n",
      "           Conv2d-16          [-1, 272, 30, 30]          26,112\n",
      "      BatchNorm2d-17          [-1, 272, 30, 30]             544\n",
      "       Bottleneck-18          [-1, 304, 30, 30]               0\n",
      "           Conv2d-19           [-1, 96, 30, 30]          29,184\n",
      "      BatchNorm2d-20           [-1, 96, 30, 30]             192\n",
      "           Conv2d-21           [-1, 96, 30, 30]           2,592\n",
      "      BatchNorm2d-22           [-1, 96, 30, 30]             192\n",
      "           Conv2d-23          [-1, 272, 30, 30]          26,112\n",
      "      BatchNorm2d-24          [-1, 272, 30, 30]             544\n",
      "       Bottleneck-25          [-1, 320, 30, 30]               0\n",
      "           Conv2d-26          [-1, 192, 30, 30]          61,440\n",
      "      BatchNorm2d-27          [-1, 192, 30, 30]             384\n",
      "           Conv2d-28          [-1, 192, 15, 15]          10,368\n",
      "      BatchNorm2d-29          [-1, 192, 15, 15]             384\n",
      "           Conv2d-30          [-1, 544, 15, 15]         104,448\n",
      "      BatchNorm2d-31          [-1, 544, 15, 15]           1,088\n",
      "           Conv2d-32          [-1, 544, 15, 15]         174,080\n",
      "      BatchNorm2d-33          [-1, 544, 15, 15]           1,088\n",
      "       Bottleneck-34          [-1, 576, 15, 15]               0\n",
      "           Conv2d-35          [-1, 192, 15, 15]         110,592\n",
      "      BatchNorm2d-36          [-1, 192, 15, 15]             384\n",
      "           Conv2d-37          [-1, 192, 15, 15]          10,368\n",
      "      BatchNorm2d-38          [-1, 192, 15, 15]             384\n",
      "           Conv2d-39          [-1, 544, 15, 15]         104,448\n",
      "      BatchNorm2d-40          [-1, 544, 15, 15]           1,088\n",
      "       Bottleneck-41          [-1, 608, 15, 15]               0\n",
      "           Conv2d-42          [-1, 192, 15, 15]         116,736\n",
      "      BatchNorm2d-43          [-1, 192, 15, 15]             384\n",
      "           Conv2d-44          [-1, 192, 15, 15]          10,368\n",
      "      BatchNorm2d-45          [-1, 192, 15, 15]             384\n",
      "           Conv2d-46          [-1, 544, 15, 15]         104,448\n",
      "      BatchNorm2d-47          [-1, 544, 15, 15]           1,088\n",
      "       Bottleneck-48          [-1, 640, 15, 15]               0\n",
      "           Conv2d-49          [-1, 192, 15, 15]         122,880\n",
      "      BatchNorm2d-50          [-1, 192, 15, 15]             384\n",
      "           Conv2d-51          [-1, 192, 15, 15]          10,368\n",
      "      BatchNorm2d-52          [-1, 192, 15, 15]             384\n",
      "           Conv2d-53          [-1, 544, 15, 15]         104,448\n",
      "      BatchNorm2d-54          [-1, 544, 15, 15]           1,088\n",
      "       Bottleneck-55          [-1, 672, 15, 15]               0\n",
      "           Conv2d-56          [-1, 384, 15, 15]         258,048\n",
      "      BatchNorm2d-57          [-1, 384, 15, 15]             768\n",
      "           Conv2d-58            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-59            [-1, 384, 8, 8]             768\n",
      "           Conv2d-60           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-61           [-1, 1048, 8, 8]           2,096\n",
      "           Conv2d-62           [-1, 1048, 8, 8]         704,256\n",
      "      BatchNorm2d-63           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-64           [-1, 1072, 8, 8]               0\n",
      "           Conv2d-65            [-1, 384, 8, 8]         411,648\n",
      "      BatchNorm2d-66            [-1, 384, 8, 8]             768\n",
      "           Conv2d-67            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-68            [-1, 384, 8, 8]             768\n",
      "           Conv2d-69           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-70           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-71           [-1, 1096, 8, 8]               0\n",
      "           Conv2d-72            [-1, 384, 8, 8]         420,864\n",
      "      BatchNorm2d-73            [-1, 384, 8, 8]             768\n",
      "           Conv2d-74            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-75            [-1, 384, 8, 8]             768\n",
      "           Conv2d-76           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-77           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-78           [-1, 1120, 8, 8]               0\n",
      "           Conv2d-79            [-1, 384, 8, 8]         430,080\n",
      "      BatchNorm2d-80            [-1, 384, 8, 8]             768\n",
      "           Conv2d-81            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-82            [-1, 384, 8, 8]             768\n",
      "           Conv2d-83           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-84           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-85           [-1, 1144, 8, 8]               0\n",
      "           Conv2d-86            [-1, 384, 8, 8]         439,296\n",
      "      BatchNorm2d-87            [-1, 384, 8, 8]             768\n",
      "           Conv2d-88            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-89            [-1, 384, 8, 8]             768\n",
      "           Conv2d-90           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-91           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-92           [-1, 1168, 8, 8]               0\n",
      "           Conv2d-93            [-1, 384, 8, 8]         448,512\n",
      "      BatchNorm2d-94            [-1, 384, 8, 8]             768\n",
      "           Conv2d-95            [-1, 384, 8, 8]          41,472\n",
      "      BatchNorm2d-96            [-1, 384, 8, 8]             768\n",
      "           Conv2d-97           [-1, 1048, 8, 8]         402,432\n",
      "      BatchNorm2d-98           [-1, 1048, 8, 8]           2,096\n",
      "       Bottleneck-99           [-1, 1192, 8, 8]               0\n",
      "          Conv2d-100            [-1, 384, 8, 8]         457,728\n",
      "     BatchNorm2d-101            [-1, 384, 8, 8]             768\n",
      "          Conv2d-102            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-103            [-1, 384, 8, 8]             768\n",
      "          Conv2d-104           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-105           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-106           [-1, 1216, 8, 8]               0\n",
      "          Conv2d-107            [-1, 384, 8, 8]         466,944\n",
      "     BatchNorm2d-108            [-1, 384, 8, 8]             768\n",
      "          Conv2d-109            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-110            [-1, 384, 8, 8]             768\n",
      "          Conv2d-111           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-112           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-113           [-1, 1240, 8, 8]               0\n",
      "          Conv2d-114            [-1, 384, 8, 8]         476,160\n",
      "     BatchNorm2d-115            [-1, 384, 8, 8]             768\n",
      "          Conv2d-116            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-117            [-1, 384, 8, 8]             768\n",
      "          Conv2d-118           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-119           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-120           [-1, 1264, 8, 8]               0\n",
      "          Conv2d-121            [-1, 384, 8, 8]         485,376\n",
      "     BatchNorm2d-122            [-1, 384, 8, 8]             768\n",
      "          Conv2d-123            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-124            [-1, 384, 8, 8]             768\n",
      "          Conv2d-125           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-126           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-127           [-1, 1288, 8, 8]               0\n",
      "          Conv2d-128            [-1, 384, 8, 8]         494,592\n",
      "     BatchNorm2d-129            [-1, 384, 8, 8]             768\n",
      "          Conv2d-130            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-131            [-1, 384, 8, 8]             768\n",
      "          Conv2d-132           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-133           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-134           [-1, 1312, 8, 8]               0\n",
      "          Conv2d-135            [-1, 384, 8, 8]         503,808\n",
      "     BatchNorm2d-136            [-1, 384, 8, 8]             768\n",
      "          Conv2d-137            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-138            [-1, 384, 8, 8]             768\n",
      "          Conv2d-139           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-140           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-141           [-1, 1336, 8, 8]               0\n",
      "          Conv2d-142            [-1, 384, 8, 8]         513,024\n",
      "     BatchNorm2d-143            [-1, 384, 8, 8]             768\n",
      "          Conv2d-144            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-145            [-1, 384, 8, 8]             768\n",
      "          Conv2d-146           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-147           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-148           [-1, 1360, 8, 8]               0\n",
      "          Conv2d-149            [-1, 384, 8, 8]         522,240\n",
      "     BatchNorm2d-150            [-1, 384, 8, 8]             768\n",
      "          Conv2d-151            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-152            [-1, 384, 8, 8]             768\n",
      "          Conv2d-153           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-154           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-155           [-1, 1384, 8, 8]               0\n",
      "          Conv2d-156            [-1, 384, 8, 8]         531,456\n",
      "     BatchNorm2d-157            [-1, 384, 8, 8]             768\n",
      "          Conv2d-158            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-159            [-1, 384, 8, 8]             768\n",
      "          Conv2d-160           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-161           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-162           [-1, 1408, 8, 8]               0\n",
      "          Conv2d-163            [-1, 384, 8, 8]         540,672\n",
      "     BatchNorm2d-164            [-1, 384, 8, 8]             768\n",
      "          Conv2d-165            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-166            [-1, 384, 8, 8]             768\n",
      "          Conv2d-167           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-168           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-169           [-1, 1432, 8, 8]               0\n",
      "          Conv2d-170            [-1, 384, 8, 8]         549,888\n",
      "     BatchNorm2d-171            [-1, 384, 8, 8]             768\n",
      "          Conv2d-172            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-173            [-1, 384, 8, 8]             768\n",
      "          Conv2d-174           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-175           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-176           [-1, 1456, 8, 8]               0\n",
      "          Conv2d-177            [-1, 384, 8, 8]         559,104\n",
      "     BatchNorm2d-178            [-1, 384, 8, 8]             768\n",
      "          Conv2d-179            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-180            [-1, 384, 8, 8]             768\n",
      "          Conv2d-181           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-182           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-183           [-1, 1480, 8, 8]               0\n",
      "          Conv2d-184            [-1, 384, 8, 8]         568,320\n",
      "     BatchNorm2d-185            [-1, 384, 8, 8]             768\n",
      "          Conv2d-186            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-187            [-1, 384, 8, 8]             768\n",
      "          Conv2d-188           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-189           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-190           [-1, 1504, 8, 8]               0\n",
      "          Conv2d-191            [-1, 384, 8, 8]         577,536\n",
      "     BatchNorm2d-192            [-1, 384, 8, 8]             768\n",
      "          Conv2d-193            [-1, 384, 8, 8]          41,472\n",
      "     BatchNorm2d-194            [-1, 384, 8, 8]             768\n",
      "          Conv2d-195           [-1, 1048, 8, 8]         402,432\n",
      "     BatchNorm2d-196           [-1, 1048, 8, 8]           2,096\n",
      "      Bottleneck-197           [-1, 1528, 8, 8]               0\n",
      "          Conv2d-198            [-1, 768, 8, 8]       1,173,504\n",
      "     BatchNorm2d-199            [-1, 768, 8, 8]           1,536\n",
      "          Conv2d-200            [-1, 768, 4, 4]         165,888\n",
      "     BatchNorm2d-201            [-1, 768, 4, 4]           1,536\n",
      "          Conv2d-202           [-1, 2176, 4, 4]       1,671,168\n",
      "     BatchNorm2d-203           [-1, 2176, 4, 4]           4,352\n",
      "          Conv2d-204           [-1, 2176, 4, 4]       3,324,928\n",
      "     BatchNorm2d-205           [-1, 2176, 4, 4]           4,352\n",
      "      Bottleneck-206           [-1, 2304, 4, 4]               0\n",
      "          Conv2d-207            [-1, 768, 4, 4]       1,769,472\n",
      "     BatchNorm2d-208            [-1, 768, 4, 4]           1,536\n",
      "          Conv2d-209            [-1, 768, 4, 4]         165,888\n",
      "     BatchNorm2d-210            [-1, 768, 4, 4]           1,536\n",
      "          Conv2d-211           [-1, 2176, 4, 4]       1,671,168\n",
      "     BatchNorm2d-212           [-1, 2176, 4, 4]           4,352\n",
      "      Bottleneck-213           [-1, 2432, 4, 4]               0\n",
      "          Conv2d-214            [-1, 768, 4, 4]       1,867,776\n",
      "     BatchNorm2d-215            [-1, 768, 4, 4]           1,536\n",
      "          Conv2d-216            [-1, 768, 4, 4]         165,888\n",
      "     BatchNorm2d-217            [-1, 768, 4, 4]           1,536\n",
      "          Conv2d-218           [-1, 2176, 4, 4]       1,671,168\n",
      "     BatchNorm2d-219           [-1, 2176, 4, 4]           4,352\n",
      "      Bottleneck-220           [-1, 2560, 4, 4]               0\n",
      "          Linear-221                 [-1, 1000]       2,561,000\n",
      "================================================================\n",
      "Total params: 36,771,896\n",
      "Trainable params: 36,771,896\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 105.71\n",
      "Params size (MB): 140.27\n",
      "Estimated Total Size (MB): 246.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''Dual Path Networks in PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.out_planes = out_planes\n",
    "        self.dense_depth = dense_depth\n",
    "\n",
    "        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if first_layer:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes+dense_depth)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        x = self.shortcut(x)\n",
    "        d = self.out_planes\n",
    "        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DPN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DPN, self).__init__()\n",
    "        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n",
    "        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.last_planes = 64\n",
    "        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n",
    "        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n",
    "        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n",
    "        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n",
    "        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 1000)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for i,stride in enumerate(strides):\n",
    "            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n",
    "            self.last_planes = out_planes + (i+2) * dense_depth\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def DPN26():\n",
    "    cfg = {\n",
    "        'in_planes': (96,192,384,768),\n",
    "        'out_planes': (256,512,1024,2048),\n",
    "        'num_blocks': (2,2,2,2),\n",
    "        'dense_depth': (16,32,24,128)\n",
    "    }\n",
    "    return DPN(cfg)\n",
    "\n",
    "def DPN92():\n",
    "    cfg = {\n",
    "        'in_planes': (96,192,384,768),\n",
    "        'out_planes': (256,512,1024,2048),\n",
    "        'num_blocks': (3,4,20,3),\n",
    "        'dense_depth': (16,32,24,128)\n",
    "    }\n",
    "    return DPN(cfg)\n",
    "\n",
    "model = DPN92()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6ba1b8e027b1f51340dff46820798cb0794560ec"
   },
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()\n",
    "#error = error.to(device)\n",
    "\n",
    "learning_rate = 0.002\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "6f203a72e519b9c110b0759ee1fcfd34fe039dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtf\n"
     ]
    }
   ],
   "source": [
    "print('wtf')\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.view(-1, 1, h, w).to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1)% 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (i+ 1) * len(images), len(train_loader.dataset),\n",
    "                100. * (i + 1) / len(train_loader), loss.item()))\n",
    "            \n",
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.view(-1, 1, h, w).to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss += F.cross_entropy(outputs, labels, size_average=False).item()\n",
    "\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "        \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "b1072e43e1d68c55266189ffd0328470b15d88f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3200/332987 (1%)]\tLoss: 6.912961\n",
      "Train Epoch: 0 [6400/332987 (2%)]\tLoss: 6.902548\n",
      "Train Epoch: 0 [9600/332987 (3%)]\tLoss: 6.906547\n",
      "Train Epoch: 0 [12800/332987 (4%)]\tLoss: 6.908698\n",
      "Train Epoch: 0 [16000/332987 (5%)]\tLoss: 6.922197\n",
      "Train Epoch: 0 [19200/332987 (6%)]\tLoss: 6.912653\n",
      "Train Epoch: 0 [22400/332987 (7%)]\tLoss: 6.902184\n",
      "Train Epoch: 0 [25600/332987 (8%)]\tLoss: 6.911507\n",
      "Train Epoch: 0 [28800/332987 (9%)]\tLoss: 6.888675\n",
      "Train Epoch: 0 [32000/332987 (10%)]\tLoss: 6.900495\n",
      "Train Epoch: 0 [35200/332987 (11%)]\tLoss: 6.876614\n",
      "Train Epoch: 0 [38400/332987 (12%)]\tLoss: 6.571973\n",
      "Train Epoch: 0 [41600/332987 (12%)]\tLoss: 6.481762\n",
      "Train Epoch: 0 [44800/332987 (13%)]\tLoss: 6.357326\n",
      "Train Epoch: 0 [48000/332987 (14%)]\tLoss: 6.484236\n",
      "Train Epoch: 0 [51200/332987 (15%)]\tLoss: 6.329052\n",
      "Train Epoch: 0 [54400/332987 (16%)]\tLoss: 6.283402\n",
      "Train Epoch: 0 [57600/332987 (17%)]\tLoss: 6.130381\n",
      "Train Epoch: 0 [60800/332987 (18%)]\tLoss: 6.093709\n",
      "Train Epoch: 0 [64000/332987 (19%)]\tLoss: 6.221331\n",
      "Train Epoch: 0 [67200/332987 (20%)]\tLoss: 5.997701\n",
      "Train Epoch: 0 [70400/332987 (21%)]\tLoss: 5.953670\n",
      "Train Epoch: 0 [73600/332987 (22%)]\tLoss: 6.059195\n",
      "Train Epoch: 0 [76800/332987 (23%)]\tLoss: 5.796216\n",
      "Train Epoch: 0 [80000/332987 (24%)]\tLoss: 5.505694\n",
      "Train Epoch: 0 [83200/332987 (25%)]\tLoss: 5.514143\n",
      "Train Epoch: 0 [86400/332987 (26%)]\tLoss: 5.421435\n",
      "Train Epoch: 0 [89600/332987 (27%)]\tLoss: 5.091897\n",
      "Train Epoch: 0 [92800/332987 (28%)]\tLoss: 4.786785\n",
      "Train Epoch: 0 [96000/332987 (29%)]\tLoss: 4.557817\n",
      "Train Epoch: 0 [99200/332987 (30%)]\tLoss: 4.239564\n",
      "Train Epoch: 0 [102400/332987 (31%)]\tLoss: 3.866353\n",
      "Train Epoch: 0 [105600/332987 (32%)]\tLoss: 3.733112\n",
      "Train Epoch: 0 [108800/332987 (33%)]\tLoss: 3.510578\n",
      "Train Epoch: 0 [112000/332987 (34%)]\tLoss: 3.011228\n",
      "Train Epoch: 0 [115200/332987 (35%)]\tLoss: 3.413456\n",
      "Train Epoch: 0 [118400/332987 (36%)]\tLoss: 2.652643\n",
      "Train Epoch: 0 [121600/332987 (37%)]\tLoss: 2.987200\n",
      "Train Epoch: 0 [124800/332987 (37%)]\tLoss: 2.582918\n",
      "Train Epoch: 0 [128000/332987 (38%)]\tLoss: 2.440586\n",
      "Train Epoch: 0 [131200/332987 (39%)]\tLoss: 2.258982\n",
      "Train Epoch: 0 [134400/332987 (40%)]\tLoss: 2.331030\n",
      "Train Epoch: 0 [137600/332987 (41%)]\tLoss: 1.755994\n",
      "Train Epoch: 0 [140800/332987 (42%)]\tLoss: 1.251406\n",
      "Train Epoch: 0 [144000/332987 (43%)]\tLoss: 1.536350\n",
      "Train Epoch: 0 [147200/332987 (44%)]\tLoss: 1.617863\n",
      "Train Epoch: 0 [150400/332987 (45%)]\tLoss: 1.350347\n",
      "Train Epoch: 0 [153600/332987 (46%)]\tLoss: 1.220733\n",
      "Train Epoch: 0 [156800/332987 (47%)]\tLoss: 1.603027\n",
      "Train Epoch: 0 [160000/332987 (48%)]\tLoss: 1.230220\n",
      "Train Epoch: 0 [163200/332987 (49%)]\tLoss: 1.049684\n",
      "Train Epoch: 0 [166400/332987 (50%)]\tLoss: 0.750012\n",
      "Train Epoch: 0 [169600/332987 (51%)]\tLoss: 0.882206\n",
      "Train Epoch: 0 [172800/332987 (52%)]\tLoss: 0.733334\n",
      "Train Epoch: 0 [176000/332987 (53%)]\tLoss: 1.022270\n",
      "Train Epoch: 0 [179200/332987 (54%)]\tLoss: 0.535415\n",
      "Train Epoch: 0 [182400/332987 (55%)]\tLoss: 0.685479\n",
      "Train Epoch: 0 [185600/332987 (56%)]\tLoss: 0.615612\n",
      "Train Epoch: 0 [188800/332987 (57%)]\tLoss: 1.154784\n",
      "Train Epoch: 0 [192000/332987 (58%)]\tLoss: 0.657115\n",
      "Train Epoch: 0 [195200/332987 (59%)]\tLoss: 0.564822\n",
      "Train Epoch: 0 [198400/332987 (60%)]\tLoss: 0.484548\n",
      "Train Epoch: 0 [201600/332987 (61%)]\tLoss: 0.622430\n",
      "Train Epoch: 0 [204800/332987 (62%)]\tLoss: 0.513244\n",
      "Train Epoch: 0 [208000/332987 (62%)]\tLoss: 0.856236\n",
      "Train Epoch: 0 [211200/332987 (63%)]\tLoss: 0.659259\n",
      "Train Epoch: 0 [214400/332987 (64%)]\tLoss: 0.362544\n",
      "Train Epoch: 0 [217600/332987 (65%)]\tLoss: 0.547481\n",
      "Train Epoch: 0 [220800/332987 (66%)]\tLoss: 0.642178\n",
      "Train Epoch: 0 [224000/332987 (67%)]\tLoss: 0.247041\n",
      "Train Epoch: 0 [227200/332987 (68%)]\tLoss: 0.505613\n",
      "Train Epoch: 0 [230400/332987 (69%)]\tLoss: 0.437215\n",
      "Train Epoch: 0 [233600/332987 (70%)]\tLoss: 0.302326\n",
      "Train Epoch: 0 [236800/332987 (71%)]\tLoss: 0.509865\n",
      "Train Epoch: 0 [240000/332987 (72%)]\tLoss: 0.406978\n",
      "Train Epoch: 0 [243200/332987 (73%)]\tLoss: 0.293860\n",
      "Train Epoch: 0 [246400/332987 (74%)]\tLoss: 0.284289\n",
      "Train Epoch: 0 [249600/332987 (75%)]\tLoss: 0.387046\n",
      "Train Epoch: 0 [252800/332987 (76%)]\tLoss: 0.343485\n",
      "Train Epoch: 0 [256000/332987 (77%)]\tLoss: 0.313100\n",
      "Train Epoch: 0 [259200/332987 (78%)]\tLoss: 0.452750\n",
      "Train Epoch: 0 [262400/332987 (79%)]\tLoss: 0.542259\n",
      "Train Epoch: 0 [265600/332987 (80%)]\tLoss: 0.482002\n",
      "Train Epoch: 0 [268800/332987 (81%)]\tLoss: 0.366923\n",
      "Train Epoch: 0 [272000/332987 (82%)]\tLoss: 0.475839\n",
      "Train Epoch: 0 [275200/332987 (83%)]\tLoss: 0.559363\n",
      "Train Epoch: 0 [278400/332987 (84%)]\tLoss: 0.415031\n",
      "Train Epoch: 0 [281600/332987 (85%)]\tLoss: 0.354637\n",
      "Train Epoch: 0 [284800/332987 (86%)]\tLoss: 0.340263\n",
      "Train Epoch: 0 [288000/332987 (86%)]\tLoss: 0.356077\n",
      "Train Epoch: 0 [291200/332987 (87%)]\tLoss: 0.726346\n",
      "Train Epoch: 0 [294400/332987 (88%)]\tLoss: 0.421795\n",
      "Train Epoch: 0 [297600/332987 (89%)]\tLoss: 0.350036\n",
      "Train Epoch: 0 [300800/332987 (90%)]\tLoss: 0.308768\n",
      "Train Epoch: 0 [304000/332987 (91%)]\tLoss: 0.520125\n",
      "Train Epoch: 0 [307200/332987 (92%)]\tLoss: 0.315772\n",
      "Train Epoch: 0 [310400/332987 (93%)]\tLoss: 0.600846\n",
      "Train Epoch: 0 [313600/332987 (94%)]\tLoss: 0.195689\n",
      "Train Epoch: 0 [316800/332987 (95%)]\tLoss: 0.637294\n",
      "Train Epoch: 0 [320000/332987 (96%)]\tLoss: 0.234745\n",
      "Train Epoch: 0 [323200/332987 (97%)]\tLoss: 0.229523\n",
      "Train Epoch: 0 [326400/332987 (98%)]\tLoss: 0.295067\n",
      "Train Epoch: 0 [329600/332987 (99%)]\tLoss: 0.296274\n",
      "Train Epoch: 0 [332800/332987 (100%)]\tLoss: 0.412570\n",
      "Train Epoch: 1 [3200/332987 (1%)]\tLoss: 0.224003\n",
      "Train Epoch: 1 [6400/332987 (2%)]\tLoss: 0.211282\n",
      "Train Epoch: 1 [9600/332987 (3%)]\tLoss: 0.123871\n",
      "Train Epoch: 1 [12800/332987 (4%)]\tLoss: 0.193945\n",
      "Train Epoch: 1 [16000/332987 (5%)]\tLoss: 0.307124\n",
      "Train Epoch: 1 [19200/332987 (6%)]\tLoss: 0.340989\n",
      "Train Epoch: 1 [22400/332987 (7%)]\tLoss: 0.141950\n",
      "Train Epoch: 1 [25600/332987 (8%)]\tLoss: 0.327239\n",
      "Train Epoch: 1 [28800/332987 (9%)]\tLoss: 0.140224\n",
      "Train Epoch: 1 [32000/332987 (10%)]\tLoss: 0.177465\n",
      "Train Epoch: 1 [35200/332987 (11%)]\tLoss: 0.102290\n",
      "Train Epoch: 1 [38400/332987 (12%)]\tLoss: 0.206942\n",
      "Train Epoch: 1 [41600/332987 (12%)]\tLoss: 0.435994\n",
      "Train Epoch: 1 [44800/332987 (13%)]\tLoss: 0.201225\n",
      "Train Epoch: 1 [48000/332987 (14%)]\tLoss: 0.245843\n",
      "Train Epoch: 1 [51200/332987 (15%)]\tLoss: 0.236097\n",
      "Train Epoch: 1 [54400/332987 (16%)]\tLoss: 0.098598\n",
      "Train Epoch: 1 [57600/332987 (17%)]\tLoss: 0.249952\n",
      "Train Epoch: 1 [60800/332987 (18%)]\tLoss: 0.226398\n",
      "Train Epoch: 1 [64000/332987 (19%)]\tLoss: 0.170236\n",
      "Train Epoch: 1 [67200/332987 (20%)]\tLoss: 0.287389\n",
      "Train Epoch: 1 [70400/332987 (21%)]\tLoss: 0.217985\n",
      "Train Epoch: 1 [73600/332987 (22%)]\tLoss: 0.203333\n",
      "Train Epoch: 1 [76800/332987 (23%)]\tLoss: 0.187374\n",
      "Train Epoch: 1 [80000/332987 (24%)]\tLoss: 0.173250\n",
      "Train Epoch: 1 [83200/332987 (25%)]\tLoss: 0.328770\n",
      "Train Epoch: 1 [86400/332987 (26%)]\tLoss: 0.280092\n",
      "Train Epoch: 1 [89600/332987 (27%)]\tLoss: 0.075522\n",
      "Train Epoch: 1 [92800/332987 (28%)]\tLoss: 0.183359\n",
      "Train Epoch: 1 [96000/332987 (29%)]\tLoss: 0.258804\n",
      "Train Epoch: 1 [99200/332987 (30%)]\tLoss: 0.399380\n",
      "Train Epoch: 1 [102400/332987 (31%)]\tLoss: 0.118562\n",
      "Train Epoch: 1 [105600/332987 (32%)]\tLoss: 0.155929\n",
      "Train Epoch: 1 [108800/332987 (33%)]\tLoss: 0.303920\n",
      "Train Epoch: 1 [112000/332987 (34%)]\tLoss: 0.126825\n",
      "Train Epoch: 1 [115200/332987 (35%)]\tLoss: 0.225362\n",
      "Train Epoch: 1 [118400/332987 (36%)]\tLoss: 0.077723\n",
      "Train Epoch: 1 [121600/332987 (37%)]\tLoss: 0.292892\n",
      "Train Epoch: 1 [124800/332987 (37%)]\tLoss: 0.145302\n",
      "Train Epoch: 1 [128000/332987 (38%)]\tLoss: 0.211196\n",
      "Train Epoch: 1 [131200/332987 (39%)]\tLoss: 0.193633\n",
      "Train Epoch: 1 [134400/332987 (40%)]\tLoss: 0.336802\n",
      "Train Epoch: 1 [137600/332987 (41%)]\tLoss: 0.218119\n",
      "Train Epoch: 1 [140800/332987 (42%)]\tLoss: 0.269535\n",
      "Train Epoch: 1 [144000/332987 (43%)]\tLoss: 0.185094\n",
      "Train Epoch: 1 [147200/332987 (44%)]\tLoss: 0.151236\n",
      "Train Epoch: 1 [150400/332987 (45%)]\tLoss: 0.222444\n",
      "Train Epoch: 1 [153600/332987 (46%)]\tLoss: 0.362873\n",
      "Train Epoch: 1 [156800/332987 (47%)]\tLoss: 0.120974\n",
      "Train Epoch: 1 [160000/332987 (48%)]\tLoss: 0.142243\n",
      "Train Epoch: 1 [163200/332987 (49%)]\tLoss: 0.042962\n",
      "Train Epoch: 1 [166400/332987 (50%)]\tLoss: 0.150414\n",
      "Train Epoch: 1 [169600/332987 (51%)]\tLoss: 0.165338\n",
      "Train Epoch: 1 [172800/332987 (52%)]\tLoss: 0.137386\n",
      "Train Epoch: 1 [176000/332987 (53%)]\tLoss: 0.102351\n",
      "Train Epoch: 1 [179200/332987 (54%)]\tLoss: 0.318938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [182400/332987 (55%)]\tLoss: 0.183904\n",
      "Train Epoch: 1 [185600/332987 (56%)]\tLoss: 0.133254\n",
      "Train Epoch: 1 [188800/332987 (57%)]\tLoss: 0.061122\n",
      "Train Epoch: 1 [192000/332987 (58%)]\tLoss: 0.254653\n",
      "Train Epoch: 1 [195200/332987 (59%)]\tLoss: 0.187228\n",
      "Train Epoch: 1 [198400/332987 (60%)]\tLoss: 0.260102\n",
      "Train Epoch: 1 [201600/332987 (61%)]\tLoss: 0.305662\n",
      "Train Epoch: 1 [204800/332987 (62%)]\tLoss: 0.269894\n",
      "Train Epoch: 1 [208000/332987 (62%)]\tLoss: 0.132259\n",
      "Train Epoch: 1 [211200/332987 (63%)]\tLoss: 0.209872\n",
      "Train Epoch: 1 [214400/332987 (64%)]\tLoss: 0.427169\n",
      "Train Epoch: 1 [217600/332987 (65%)]\tLoss: 0.206499\n",
      "Train Epoch: 1 [220800/332987 (66%)]\tLoss: 0.193435\n",
      "Train Epoch: 1 [224000/332987 (67%)]\tLoss: 0.136257\n",
      "Train Epoch: 1 [227200/332987 (68%)]\tLoss: 0.405400\n",
      "Train Epoch: 1 [230400/332987 (69%)]\tLoss: 0.161874\n",
      "Train Epoch: 1 [233600/332987 (70%)]\tLoss: 0.179220\n",
      "Train Epoch: 1 [236800/332987 (71%)]\tLoss: 0.185250\n",
      "Train Epoch: 1 [240000/332987 (72%)]\tLoss: 0.410193\n",
      "Train Epoch: 1 [243200/332987 (73%)]\tLoss: 0.178367\n",
      "Train Epoch: 1 [246400/332987 (74%)]\tLoss: 0.124123\n",
      "Train Epoch: 1 [249600/332987 (75%)]\tLoss: 0.305846\n",
      "Train Epoch: 1 [252800/332987 (76%)]\tLoss: 0.110614\n",
      "Train Epoch: 1 [256000/332987 (77%)]\tLoss: 0.235139\n",
      "Train Epoch: 1 [259200/332987 (78%)]\tLoss: 0.172560\n",
      "Train Epoch: 1 [262400/332987 (79%)]\tLoss: 0.129505\n",
      "Train Epoch: 1 [265600/332987 (80%)]\tLoss: 0.115703\n",
      "Train Epoch: 1 [268800/332987 (81%)]\tLoss: 0.074211\n",
      "Train Epoch: 1 [272000/332987 (82%)]\tLoss: 0.243272\n",
      "Train Epoch: 1 [275200/332987 (83%)]\tLoss: 0.264242\n",
      "Train Epoch: 1 [278400/332987 (84%)]\tLoss: 0.307827\n",
      "Train Epoch: 1 [281600/332987 (85%)]\tLoss: 0.085190\n",
      "Train Epoch: 1 [284800/332987 (86%)]\tLoss: 0.070978\n",
      "Train Epoch: 1 [288000/332987 (86%)]\tLoss: 0.089710\n",
      "Train Epoch: 1 [291200/332987 (87%)]\tLoss: 0.069095\n",
      "Train Epoch: 1 [294400/332987 (88%)]\tLoss: 0.114578\n",
      "Train Epoch: 1 [297600/332987 (89%)]\tLoss: 0.212145\n",
      "Train Epoch: 1 [300800/332987 (90%)]\tLoss: 0.196207\n",
      "Train Epoch: 1 [304000/332987 (91%)]\tLoss: 0.084205\n",
      "Train Epoch: 1 [307200/332987 (92%)]\tLoss: 0.307632\n",
      "Train Epoch: 1 [310400/332987 (93%)]\tLoss: 0.128595\n",
      "Train Epoch: 1 [313600/332987 (94%)]\tLoss: 0.057588\n",
      "Train Epoch: 1 [316800/332987 (95%)]\tLoss: 0.177903\n",
      "Train Epoch: 1 [320000/332987 (96%)]\tLoss: 0.302114\n",
      "Train Epoch: 1 [323200/332987 (97%)]\tLoss: 0.080425\n",
      "Train Epoch: 1 [326400/332987 (98%)]\tLoss: 0.273048\n",
      "Train Epoch: 1 [329600/332987 (99%)]\tLoss: 0.195948\n",
      "Train Epoch: 1 [332800/332987 (100%)]\tLoss: 0.131287\n",
      "Train Epoch: 2 [3200/332987 (1%)]\tLoss: 0.061498\n",
      "Train Epoch: 2 [6400/332987 (2%)]\tLoss: 0.112828\n",
      "Train Epoch: 2 [9600/332987 (3%)]\tLoss: 0.136035\n",
      "Train Epoch: 2 [12800/332987 (4%)]\tLoss: 0.149606\n",
      "Train Epoch: 2 [16000/332987 (5%)]\tLoss: 0.088666\n",
      "Train Epoch: 2 [19200/332987 (6%)]\tLoss: 0.161560\n",
      "Train Epoch: 2 [22400/332987 (7%)]\tLoss: 0.058016\n",
      "Train Epoch: 2 [25600/332987 (8%)]\tLoss: 0.182772\n",
      "Train Epoch: 2 [28800/332987 (9%)]\tLoss: 0.058999\n",
      "Train Epoch: 2 [32000/332987 (10%)]\tLoss: 0.017992\n",
      "Train Epoch: 2 [35200/332987 (11%)]\tLoss: 0.065571\n",
      "Train Epoch: 2 [38400/332987 (12%)]\tLoss: 0.163054\n",
      "Train Epoch: 2 [41600/332987 (12%)]\tLoss: 0.225848\n",
      "Train Epoch: 2 [44800/332987 (13%)]\tLoss: 0.010803\n",
      "Train Epoch: 2 [48000/332987 (14%)]\tLoss: 0.094243\n",
      "Train Epoch: 2 [51200/332987 (15%)]\tLoss: 0.106904\n",
      "Train Epoch: 2 [54400/332987 (16%)]\tLoss: 0.018014\n",
      "Train Epoch: 2 [57600/332987 (17%)]\tLoss: 0.072055\n",
      "Train Epoch: 2 [60800/332987 (18%)]\tLoss: 0.119625\n",
      "Train Epoch: 2 [64000/332987 (19%)]\tLoss: 0.200832\n",
      "Train Epoch: 2 [67200/332987 (20%)]\tLoss: 0.066609\n",
      "Train Epoch: 2 [70400/332987 (21%)]\tLoss: 0.036889\n",
      "Train Epoch: 2 [73600/332987 (22%)]\tLoss: 0.156167\n",
      "Train Epoch: 2 [76800/332987 (23%)]\tLoss: 0.028315\n",
      "Train Epoch: 2 [80000/332987 (24%)]\tLoss: 0.186406\n",
      "Train Epoch: 2 [83200/332987 (25%)]\tLoss: 0.116227\n",
      "Train Epoch: 2 [86400/332987 (26%)]\tLoss: 0.045771\n",
      "Train Epoch: 2 [89600/332987 (27%)]\tLoss: 0.088625\n",
      "Train Epoch: 2 [92800/332987 (28%)]\tLoss: 0.007948\n",
      "Train Epoch: 2 [96000/332987 (29%)]\tLoss: 0.139166\n",
      "Train Epoch: 2 [99200/332987 (30%)]\tLoss: 0.078402\n",
      "Train Epoch: 2 [102400/332987 (31%)]\tLoss: 0.057192\n",
      "Train Epoch: 2 [105600/332987 (32%)]\tLoss: 0.142254\n",
      "Train Epoch: 2 [108800/332987 (33%)]\tLoss: 0.109208\n",
      "Train Epoch: 2 [112000/332987 (34%)]\tLoss: 0.037841\n",
      "Train Epoch: 2 [115200/332987 (35%)]\tLoss: 0.025234\n",
      "Train Epoch: 2 [118400/332987 (36%)]\tLoss: 0.090467\n",
      "Train Epoch: 2 [121600/332987 (37%)]\tLoss: 0.017921\n",
      "Train Epoch: 2 [124800/332987 (37%)]\tLoss: 0.084381\n",
      "Train Epoch: 2 [128000/332987 (38%)]\tLoss: 0.077766\n",
      "Train Epoch: 2 [131200/332987 (39%)]\tLoss: 0.027445\n",
      "Train Epoch: 2 [134400/332987 (40%)]\tLoss: 0.089387\n",
      "Train Epoch: 2 [137600/332987 (41%)]\tLoss: 0.086429\n",
      "Train Epoch: 2 [140800/332987 (42%)]\tLoss: 0.056954\n",
      "Train Epoch: 2 [144000/332987 (43%)]\tLoss: 0.045219\n",
      "Train Epoch: 2 [147200/332987 (44%)]\tLoss: 0.030139\n",
      "Train Epoch: 2 [150400/332987 (45%)]\tLoss: 0.016735\n",
      "Train Epoch: 2 [153600/332987 (46%)]\tLoss: 0.028971\n",
      "Train Epoch: 2 [156800/332987 (47%)]\tLoss: 0.036222\n",
      "Train Epoch: 2 [160000/332987 (48%)]\tLoss: 0.258706\n",
      "Train Epoch: 2 [163200/332987 (49%)]\tLoss: 0.124710\n",
      "Train Epoch: 2 [166400/332987 (50%)]\tLoss: 0.015708\n",
      "Train Epoch: 2 [169600/332987 (51%)]\tLoss: 0.065731\n",
      "Train Epoch: 2 [172800/332987 (52%)]\tLoss: 0.051483\n",
      "Train Epoch: 2 [176000/332987 (53%)]\tLoss: 0.036244\n",
      "Train Epoch: 2 [179200/332987 (54%)]\tLoss: 0.042875\n",
      "Train Epoch: 2 [182400/332987 (55%)]\tLoss: 0.029788\n",
      "Train Epoch: 2 [185600/332987 (56%)]\tLoss: 0.096096\n",
      "Train Epoch: 2 [188800/332987 (57%)]\tLoss: 0.035490\n",
      "Train Epoch: 2 [192000/332987 (58%)]\tLoss: 0.020770\n",
      "Train Epoch: 2 [195200/332987 (59%)]\tLoss: 0.076087\n",
      "Train Epoch: 2 [198400/332987 (60%)]\tLoss: 0.058868\n",
      "Train Epoch: 2 [201600/332987 (61%)]\tLoss: 0.090014\n",
      "Train Epoch: 2 [204800/332987 (62%)]\tLoss: 0.091682\n",
      "Train Epoch: 2 [208000/332987 (62%)]\tLoss: 0.026223\n",
      "Train Epoch: 2 [211200/332987 (63%)]\tLoss: 0.026120\n",
      "Train Epoch: 2 [214400/332987 (64%)]\tLoss: 0.121154\n",
      "Train Epoch: 2 [217600/332987 (65%)]\tLoss: 0.026264\n",
      "Train Epoch: 2 [220800/332987 (66%)]\tLoss: 0.015203\n",
      "Train Epoch: 2 [224000/332987 (67%)]\tLoss: 0.087502\n",
      "Train Epoch: 2 [227200/332987 (68%)]\tLoss: 0.080094\n",
      "Train Epoch: 2 [230400/332987 (69%)]\tLoss: 0.095846\n",
      "Train Epoch: 2 [233600/332987 (70%)]\tLoss: 0.032798\n",
      "Train Epoch: 2 [236800/332987 (71%)]\tLoss: 0.035589\n",
      "Train Epoch: 2 [240000/332987 (72%)]\tLoss: 0.042943\n",
      "Train Epoch: 2 [243200/332987 (73%)]\tLoss: 0.008509\n",
      "Train Epoch: 2 [246400/332987 (74%)]\tLoss: 0.029313\n",
      "Train Epoch: 2 [249600/332987 (75%)]\tLoss: 0.030591\n",
      "Train Epoch: 2 [252800/332987 (76%)]\tLoss: 0.090801\n",
      "Train Epoch: 2 [256000/332987 (77%)]\tLoss: 0.038396\n",
      "Train Epoch: 2 [259200/332987 (78%)]\tLoss: 0.035581\n",
      "Train Epoch: 2 [262400/332987 (79%)]\tLoss: 0.027600\n",
      "Train Epoch: 2 [265600/332987 (80%)]\tLoss: 0.065416\n",
      "Train Epoch: 2 [268800/332987 (81%)]\tLoss: 0.032389\n",
      "Train Epoch: 2 [272000/332987 (82%)]\tLoss: 0.037050\n",
      "Train Epoch: 2 [275200/332987 (83%)]\tLoss: 0.035145\n",
      "Train Epoch: 2 [278400/332987 (84%)]\tLoss: 0.062449\n",
      "Train Epoch: 2 [281600/332987 (85%)]\tLoss: 0.023103\n",
      "Train Epoch: 2 [284800/332987 (86%)]\tLoss: 0.039447\n",
      "Train Epoch: 2 [288000/332987 (86%)]\tLoss: 0.162972\n",
      "Train Epoch: 2 [291200/332987 (87%)]\tLoss: 0.133651\n",
      "Train Epoch: 2 [294400/332987 (88%)]\tLoss: 0.061243\n",
      "Train Epoch: 2 [297600/332987 (89%)]\tLoss: 0.032036\n",
      "Train Epoch: 2 [300800/332987 (90%)]\tLoss: 0.103611\n",
      "Train Epoch: 2 [304000/332987 (91%)]\tLoss: 0.025822\n",
      "Train Epoch: 2 [307200/332987 (92%)]\tLoss: 0.015625\n",
      "Train Epoch: 2 [310400/332987 (93%)]\tLoss: 0.099965\n",
      "Train Epoch: 2 [313600/332987 (94%)]\tLoss: 0.044332\n",
      "Train Epoch: 2 [316800/332987 (95%)]\tLoss: 0.084152\n",
      "Train Epoch: 2 [320000/332987 (96%)]\tLoss: 0.109636\n",
      "Train Epoch: 2 [323200/332987 (97%)]\tLoss: 0.138104\n",
      "Train Epoch: 2 [326400/332987 (98%)]\tLoss: 0.062657\n",
      "Train Epoch: 2 [329600/332987 (99%)]\tLoss: 0.063473\n",
      "Train Epoch: 2 [332800/332987 (100%)]\tLoss: 0.052787\n",
      "Train Epoch: 3 [3200/332987 (1%)]\tLoss: 0.009171\n",
      "Train Epoch: 3 [6400/332987 (2%)]\tLoss: 0.015497\n",
      "Train Epoch: 3 [9600/332987 (3%)]\tLoss: 0.061587\n",
      "Train Epoch: 3 [12800/332987 (4%)]\tLoss: 0.066206\n",
      "Train Epoch: 3 [16000/332987 (5%)]\tLoss: 0.068291\n",
      "Train Epoch: 3 [19200/332987 (6%)]\tLoss: 0.170610\n",
      "Train Epoch: 3 [22400/332987 (7%)]\tLoss: 0.021931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [25600/332987 (8%)]\tLoss: 0.026154\n",
      "Train Epoch: 3 [28800/332987 (9%)]\tLoss: 0.006249\n",
      "Train Epoch: 3 [32000/332987 (10%)]\tLoss: 0.068972\n",
      "Train Epoch: 3 [35200/332987 (11%)]\tLoss: 0.085608\n",
      "Train Epoch: 3 [38400/332987 (12%)]\tLoss: 0.056869\n",
      "Train Epoch: 3 [41600/332987 (12%)]\tLoss: 0.027435\n",
      "Train Epoch: 3 [44800/332987 (13%)]\tLoss: 0.078458\n",
      "Train Epoch: 3 [48000/332987 (14%)]\tLoss: 0.034140\n",
      "Train Epoch: 3 [51200/332987 (15%)]\tLoss: 0.052429\n",
      "Train Epoch: 3 [54400/332987 (16%)]\tLoss: 0.011962\n",
      "Train Epoch: 3 [57600/332987 (17%)]\tLoss: 0.048658\n",
      "Train Epoch: 3 [60800/332987 (18%)]\tLoss: 0.048390\n",
      "Train Epoch: 3 [64000/332987 (19%)]\tLoss: 0.068798\n",
      "Train Epoch: 3 [67200/332987 (20%)]\tLoss: 0.069478\n",
      "Train Epoch: 3 [70400/332987 (21%)]\tLoss: 0.100150\n",
      "Train Epoch: 3 [73600/332987 (22%)]\tLoss: 0.131322\n",
      "Train Epoch: 3 [76800/332987 (23%)]\tLoss: 0.110854\n",
      "Train Epoch: 3 [80000/332987 (24%)]\tLoss: 0.014543\n",
      "Train Epoch: 3 [83200/332987 (25%)]\tLoss: 0.051788\n",
      "Train Epoch: 3 [86400/332987 (26%)]\tLoss: 0.060929\n",
      "Train Epoch: 3 [89600/332987 (27%)]\tLoss: 0.084177\n",
      "Train Epoch: 3 [92800/332987 (28%)]\tLoss: 0.005288\n",
      "Train Epoch: 3 [96000/332987 (29%)]\tLoss: 0.013557\n",
      "Train Epoch: 3 [99200/332987 (30%)]\tLoss: 0.084027\n",
      "Train Epoch: 3 [102400/332987 (31%)]\tLoss: 0.173395\n",
      "Train Epoch: 3 [105600/332987 (32%)]\tLoss: 0.106509\n",
      "Train Epoch: 3 [108800/332987 (33%)]\tLoss: 0.051060\n",
      "Train Epoch: 3 [112000/332987 (34%)]\tLoss: 0.120043\n",
      "Train Epoch: 3 [115200/332987 (35%)]\tLoss: 0.023022\n",
      "Train Epoch: 3 [118400/332987 (36%)]\tLoss: 0.045843\n",
      "Train Epoch: 3 [121600/332987 (37%)]\tLoss: 0.006432\n",
      "Train Epoch: 3 [124800/332987 (37%)]\tLoss: 0.062889\n",
      "Train Epoch: 3 [128000/332987 (38%)]\tLoss: 0.084139\n",
      "Train Epoch: 3 [131200/332987 (39%)]\tLoss: 0.012973\n",
      "Train Epoch: 3 [134400/332987 (40%)]\tLoss: 0.107270\n",
      "Train Epoch: 3 [137600/332987 (41%)]\tLoss: 0.063930\n",
      "Train Epoch: 3 [140800/332987 (42%)]\tLoss: 0.022346\n",
      "Train Epoch: 3 [144000/332987 (43%)]\tLoss: 0.095256\n",
      "Train Epoch: 3 [147200/332987 (44%)]\tLoss: 0.022490\n",
      "Train Epoch: 3 [150400/332987 (45%)]\tLoss: 0.035112\n",
      "Train Epoch: 3 [153600/332987 (46%)]\tLoss: 0.092773\n",
      "Train Epoch: 3 [156800/332987 (47%)]\tLoss: 0.106407\n",
      "Train Epoch: 3 [160000/332987 (48%)]\tLoss: 0.111325\n",
      "Train Epoch: 3 [163200/332987 (49%)]\tLoss: 0.104653\n",
      "Train Epoch: 3 [166400/332987 (50%)]\tLoss: 0.016619\n",
      "Train Epoch: 3 [169600/332987 (51%)]\tLoss: 0.035989\n",
      "Train Epoch: 3 [172800/332987 (52%)]\tLoss: 0.016331\n",
      "Train Epoch: 3 [176000/332987 (53%)]\tLoss: 0.050886\n",
      "Train Epoch: 3 [179200/332987 (54%)]\tLoss: 0.010811\n",
      "Train Epoch: 3 [182400/332987 (55%)]\tLoss: 0.027238\n",
      "Train Epoch: 3 [185600/332987 (56%)]\tLoss: 0.036507\n",
      "Train Epoch: 3 [188800/332987 (57%)]\tLoss: 0.152136\n",
      "Train Epoch: 3 [192000/332987 (58%)]\tLoss: 0.069081\n",
      "Train Epoch: 3 [195200/332987 (59%)]\tLoss: 0.035082\n",
      "Train Epoch: 3 [198400/332987 (60%)]\tLoss: 0.012277\n",
      "Train Epoch: 3 [201600/332987 (61%)]\tLoss: 0.008121\n",
      "Train Epoch: 3 [204800/332987 (62%)]\tLoss: 0.066771\n",
      "Train Epoch: 3 [208000/332987 (62%)]\tLoss: 0.033991\n",
      "Train Epoch: 3 [211200/332987 (63%)]\tLoss: 0.017872\n",
      "Train Epoch: 3 [214400/332987 (64%)]\tLoss: 0.038553\n",
      "Train Epoch: 3 [217600/332987 (65%)]\tLoss: 0.041450\n",
      "Train Epoch: 3 [220800/332987 (66%)]\tLoss: 0.072475\n",
      "Train Epoch: 3 [224000/332987 (67%)]\tLoss: 0.080271\n",
      "Train Epoch: 3 [227200/332987 (68%)]\tLoss: 0.133870\n",
      "Train Epoch: 3 [230400/332987 (69%)]\tLoss: 0.016162\n",
      "Train Epoch: 3 [233600/332987 (70%)]\tLoss: 0.070029\n",
      "Train Epoch: 3 [236800/332987 (71%)]\tLoss: 0.086786\n",
      "Train Epoch: 3 [240000/332987 (72%)]\tLoss: 0.019306\n",
      "Train Epoch: 3 [243200/332987 (73%)]\tLoss: 0.040042\n",
      "Train Epoch: 3 [246400/332987 (74%)]\tLoss: 0.065467\n",
      "Train Epoch: 3 [249600/332987 (75%)]\tLoss: 0.008158\n",
      "Train Epoch: 3 [252800/332987 (76%)]\tLoss: 0.132905\n",
      "Train Epoch: 3 [256000/332987 (77%)]\tLoss: 0.081466\n",
      "Train Epoch: 3 [259200/332987 (78%)]\tLoss: 0.050797\n",
      "Train Epoch: 3 [262400/332987 (79%)]\tLoss: 0.055866\n",
      "Train Epoch: 3 [265600/332987 (80%)]\tLoss: 0.158520\n",
      "Train Epoch: 3 [268800/332987 (81%)]\tLoss: 0.026080\n",
      "Train Epoch: 3 [272000/332987 (82%)]\tLoss: 0.012939\n",
      "Train Epoch: 3 [275200/332987 (83%)]\tLoss: 0.076072\n",
      "Train Epoch: 3 [278400/332987 (84%)]\tLoss: 0.127668\n",
      "Train Epoch: 3 [281600/332987 (85%)]\tLoss: 0.137453\n",
      "Train Epoch: 3 [284800/332987 (86%)]\tLoss: 0.048338\n",
      "Train Epoch: 3 [288000/332987 (86%)]\tLoss: 0.019948\n",
      "Train Epoch: 3 [291200/332987 (87%)]\tLoss: 0.010258\n",
      "Train Epoch: 3 [294400/332987 (88%)]\tLoss: 0.062335\n",
      "Train Epoch: 3 [297600/332987 (89%)]\tLoss: 0.061718\n",
      "Train Epoch: 3 [300800/332987 (90%)]\tLoss: 0.048738\n",
      "Train Epoch: 3 [304000/332987 (91%)]\tLoss: 0.084305\n",
      "Train Epoch: 3 [307200/332987 (92%)]\tLoss: 0.069872\n",
      "Train Epoch: 3 [310400/332987 (93%)]\tLoss: 0.008938\n",
      "Train Epoch: 3 [313600/332987 (94%)]\tLoss: 0.039583\n",
      "Train Epoch: 3 [316800/332987 (95%)]\tLoss: 0.063802\n",
      "Train Epoch: 3 [320000/332987 (96%)]\tLoss: 0.028089\n",
      "Train Epoch: 3 [323200/332987 (97%)]\tLoss: 0.101425\n",
      "Train Epoch: 3 [326400/332987 (98%)]\tLoss: 0.085094\n",
      "Train Epoch: 3 [329600/332987 (99%)]\tLoss: 0.028801\n",
      "Train Epoch: 3 [332800/332987 (100%)]\tLoss: 0.018580\n",
      "Train Epoch: 4 [3200/332987 (1%)]\tLoss: 0.016658\n",
      "Train Epoch: 4 [6400/332987 (2%)]\tLoss: 0.006154\n",
      "Train Epoch: 4 [9600/332987 (3%)]\tLoss: 0.030837\n",
      "Train Epoch: 4 [12800/332987 (4%)]\tLoss: 0.031491\n",
      "Train Epoch: 4 [16000/332987 (5%)]\tLoss: 0.002987\n",
      "Train Epoch: 4 [19200/332987 (6%)]\tLoss: 0.019587\n",
      "Train Epoch: 4 [22400/332987 (7%)]\tLoss: 0.012318\n",
      "Train Epoch: 4 [25600/332987 (8%)]\tLoss: 0.016397\n",
      "Train Epoch: 4 [28800/332987 (9%)]\tLoss: 0.034290\n",
      "Train Epoch: 4 [32000/332987 (10%)]\tLoss: 0.052107\n",
      "Train Epoch: 4 [35200/332987 (11%)]\tLoss: 0.003324\n",
      "Train Epoch: 4 [38400/332987 (12%)]\tLoss: 0.001650\n",
      "Train Epoch: 4 [41600/332987 (12%)]\tLoss: 0.021099\n",
      "Train Epoch: 4 [44800/332987 (13%)]\tLoss: 0.060093\n",
      "Train Epoch: 4 [48000/332987 (14%)]\tLoss: 0.007353\n",
      "Train Epoch: 4 [51200/332987 (15%)]\tLoss: 0.018144\n",
      "Train Epoch: 4 [54400/332987 (16%)]\tLoss: 0.025395\n",
      "Train Epoch: 4 [57600/332987 (17%)]\tLoss: 0.055116\n",
      "Train Epoch: 4 [60800/332987 (18%)]\tLoss: 0.072966\n",
      "Train Epoch: 4 [64000/332987 (19%)]\tLoss: 0.040538\n",
      "Train Epoch: 4 [67200/332987 (20%)]\tLoss: 0.062168\n",
      "Train Epoch: 4 [70400/332987 (21%)]\tLoss: 0.008344\n",
      "Train Epoch: 4 [73600/332987 (22%)]\tLoss: 0.003917\n",
      "Train Epoch: 4 [76800/332987 (23%)]\tLoss: 0.027279\n",
      "Train Epoch: 4 [80000/332987 (24%)]\tLoss: 0.043663\n",
      "Train Epoch: 4 [83200/332987 (25%)]\tLoss: 0.198070\n",
      "Train Epoch: 4 [86400/332987 (26%)]\tLoss: 0.030213\n",
      "Train Epoch: 4 [89600/332987 (27%)]\tLoss: 0.005620\n",
      "Train Epoch: 4 [92800/332987 (28%)]\tLoss: 0.006447\n",
      "Train Epoch: 4 [96000/332987 (29%)]\tLoss: 0.044500\n",
      "Train Epoch: 4 [99200/332987 (30%)]\tLoss: 0.019481\n",
      "Train Epoch: 4 [102400/332987 (31%)]\tLoss: 0.114338\n",
      "Train Epoch: 4 [105600/332987 (32%)]\tLoss: 0.047548\n",
      "Train Epoch: 4 [108800/332987 (33%)]\tLoss: 0.003560\n",
      "Train Epoch: 4 [112000/332987 (34%)]\tLoss: 0.010827\n",
      "Train Epoch: 4 [115200/332987 (35%)]\tLoss: 0.004847\n",
      "Train Epoch: 4 [118400/332987 (36%)]\tLoss: 0.045150\n",
      "Train Epoch: 4 [121600/332987 (37%)]\tLoss: 0.014115\n",
      "Train Epoch: 4 [124800/332987 (37%)]\tLoss: 0.017871\n",
      "Train Epoch: 4 [128000/332987 (38%)]\tLoss: 0.056860\n",
      "Train Epoch: 4 [131200/332987 (39%)]\tLoss: 0.002749\n",
      "Train Epoch: 4 [134400/332987 (40%)]\tLoss: 0.049142\n",
      "Train Epoch: 4 [137600/332987 (41%)]\tLoss: 0.005775\n",
      "Train Epoch: 4 [140800/332987 (42%)]\tLoss: 0.092973\n",
      "Train Epoch: 4 [144000/332987 (43%)]\tLoss: 0.069423\n",
      "Train Epoch: 4 [147200/332987 (44%)]\tLoss: 0.003365\n",
      "Train Epoch: 4 [150400/332987 (45%)]\tLoss: 0.004402\n",
      "Train Epoch: 4 [153600/332987 (46%)]\tLoss: 0.090740\n",
      "Train Epoch: 4 [156800/332987 (47%)]\tLoss: 0.037054\n",
      "Train Epoch: 4 [160000/332987 (48%)]\tLoss: 0.044689\n",
      "Train Epoch: 4 [163200/332987 (49%)]\tLoss: 0.041241\n",
      "Train Epoch: 4 [166400/332987 (50%)]\tLoss: 0.187588\n",
      "Train Epoch: 4 [169600/332987 (51%)]\tLoss: 0.010270\n",
      "Train Epoch: 4 [172800/332987 (52%)]\tLoss: 0.001875\n",
      "Train Epoch: 4 [176000/332987 (53%)]\tLoss: 0.001197\n",
      "Train Epoch: 4 [179200/332987 (54%)]\tLoss: 0.016294\n",
      "Train Epoch: 4 [182400/332987 (55%)]\tLoss: 0.009613\n",
      "Train Epoch: 4 [185600/332987 (56%)]\tLoss: 0.005875\n",
      "Train Epoch: 4 [188800/332987 (57%)]\tLoss: 0.026654\n",
      "Train Epoch: 4 [192000/332987 (58%)]\tLoss: 0.006332\n",
      "Train Epoch: 4 [195200/332987 (59%)]\tLoss: 0.025294\n",
      "Train Epoch: 4 [198400/332987 (60%)]\tLoss: 0.015959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [201600/332987 (61%)]\tLoss: 0.006530\n",
      "Train Epoch: 4 [204800/332987 (62%)]\tLoss: 0.013759\n",
      "Train Epoch: 4 [208000/332987 (62%)]\tLoss: 0.003928\n",
      "Train Epoch: 4 [211200/332987 (63%)]\tLoss: 0.018783\n",
      "Train Epoch: 4 [214400/332987 (64%)]\tLoss: 0.024181\n",
      "Train Epoch: 4 [217600/332987 (65%)]\tLoss: 0.005473\n",
      "Train Epoch: 4 [220800/332987 (66%)]\tLoss: 0.029957\n",
      "Train Epoch: 4 [224000/332987 (67%)]\tLoss: 0.038422\n",
      "Train Epoch: 4 [227200/332987 (68%)]\tLoss: 0.086862\n",
      "Train Epoch: 4 [230400/332987 (69%)]\tLoss: 0.008583\n",
      "Train Epoch: 4 [233600/332987 (70%)]\tLoss: 0.035062\n",
      "Train Epoch: 4 [236800/332987 (71%)]\tLoss: 0.001783\n",
      "Train Epoch: 4 [240000/332987 (72%)]\tLoss: 0.023192\n",
      "Train Epoch: 4 [243200/332987 (73%)]\tLoss: 0.000731\n",
      "Train Epoch: 4 [246400/332987 (74%)]\tLoss: 0.010067\n",
      "Train Epoch: 4 [249600/332987 (75%)]\tLoss: 0.148109\n",
      "Train Epoch: 4 [252800/332987 (76%)]\tLoss: 0.010359\n",
      "Train Epoch: 4 [256000/332987 (77%)]\tLoss: 0.018811\n",
      "Train Epoch: 4 [259200/332987 (78%)]\tLoss: 0.030441\n",
      "Train Epoch: 4 [262400/332987 (79%)]\tLoss: 0.025491\n",
      "Train Epoch: 4 [265600/332987 (80%)]\tLoss: 0.128448\n",
      "Train Epoch: 4 [268800/332987 (81%)]\tLoss: 0.021860\n",
      "Train Epoch: 4 [272000/332987 (82%)]\tLoss: 0.018778\n",
      "Train Epoch: 4 [275200/332987 (83%)]\tLoss: 0.005104\n",
      "Train Epoch: 4 [278400/332987 (84%)]\tLoss: 0.002161\n",
      "Train Epoch: 4 [281600/332987 (85%)]\tLoss: 0.007885\n",
      "Train Epoch: 4 [284800/332987 (86%)]\tLoss: 0.001349\n",
      "Train Epoch: 4 [288000/332987 (86%)]\tLoss: 0.004962\n",
      "Train Epoch: 4 [291200/332987 (87%)]\tLoss: 0.007013\n",
      "Train Epoch: 4 [294400/332987 (88%)]\tLoss: 0.023588\n",
      "Train Epoch: 4 [297600/332987 (89%)]\tLoss: 0.018062\n",
      "Train Epoch: 4 [300800/332987 (90%)]\tLoss: 0.011621\n",
      "Train Epoch: 4 [304000/332987 (91%)]\tLoss: 0.003920\n",
      "Train Epoch: 4 [307200/332987 (92%)]\tLoss: 0.006825\n",
      "Train Epoch: 4 [310400/332987 (93%)]\tLoss: 0.004864\n",
      "Train Epoch: 4 [313600/332987 (94%)]\tLoss: 0.022879\n",
      "Train Epoch: 4 [316800/332987 (95%)]\tLoss: 0.008276\n",
      "Train Epoch: 4 [320000/332987 (96%)]\tLoss: 0.003705\n",
      "Train Epoch: 4 [323200/332987 (97%)]\tLoss: 0.105261\n",
      "Train Epoch: 4 [326400/332987 (98%)]\tLoss: 0.001610\n",
      "Train Epoch: 4 [329600/332987 (99%)]\tLoss: 0.007878\n",
      "Train Epoch: 4 [332800/332987 (100%)]\tLoss: 0.017806\n",
      "Train Epoch: 5 [3200/332987 (1%)]\tLoss: 0.019800\n",
      "Train Epoch: 5 [6400/332987 (2%)]\tLoss: 0.005852\n",
      "Train Epoch: 5 [9600/332987 (3%)]\tLoss: 0.012960\n",
      "Train Epoch: 5 [12800/332987 (4%)]\tLoss: 0.013154\n",
      "Train Epoch: 5 [16000/332987 (5%)]\tLoss: 0.023690\n",
      "Train Epoch: 5 [19200/332987 (6%)]\tLoss: 0.049974\n",
      "Train Epoch: 5 [22400/332987 (7%)]\tLoss: 0.045675\n",
      "Train Epoch: 5 [25600/332987 (8%)]\tLoss: 0.020054\n",
      "Train Epoch: 5 [28800/332987 (9%)]\tLoss: 0.009824\n",
      "Train Epoch: 5 [32000/332987 (10%)]\tLoss: 0.078230\n",
      "Train Epoch: 5 [35200/332987 (11%)]\tLoss: 0.076963\n",
      "Train Epoch: 5 [38400/332987 (12%)]\tLoss: 0.014882\n",
      "Train Epoch: 5 [41600/332987 (12%)]\tLoss: 0.005464\n",
      "Train Epoch: 5 [44800/332987 (13%)]\tLoss: 0.032277\n",
      "Train Epoch: 5 [48000/332987 (14%)]\tLoss: 0.019681\n",
      "Train Epoch: 5 [51200/332987 (15%)]\tLoss: 0.031273\n",
      "Train Epoch: 5 [54400/332987 (16%)]\tLoss: 0.008582\n",
      "Train Epoch: 5 [57600/332987 (17%)]\tLoss: 0.003450\n",
      "Train Epoch: 5 [60800/332987 (18%)]\tLoss: 0.094411\n",
      "Train Epoch: 5 [64000/332987 (19%)]\tLoss: 0.006454\n",
      "Train Epoch: 5 [67200/332987 (20%)]\tLoss: 0.052047\n",
      "Train Epoch: 5 [70400/332987 (21%)]\tLoss: 0.042834\n",
      "Train Epoch: 5 [73600/332987 (22%)]\tLoss: 0.001879\n",
      "Train Epoch: 5 [76800/332987 (23%)]\tLoss: 0.010508\n",
      "Train Epoch: 5 [80000/332987 (24%)]\tLoss: 0.053505\n",
      "Train Epoch: 5 [83200/332987 (25%)]\tLoss: 0.007779\n",
      "Train Epoch: 5 [86400/332987 (26%)]\tLoss: 0.027207\n",
      "Train Epoch: 5 [89600/332987 (27%)]\tLoss: 0.001950\n",
      "Train Epoch: 5 [92800/332987 (28%)]\tLoss: 0.017132\n",
      "Train Epoch: 5 [96000/332987 (29%)]\tLoss: 0.043510\n",
      "Train Epoch: 5 [99200/332987 (30%)]\tLoss: 0.012301\n",
      "Train Epoch: 5 [102400/332987 (31%)]\tLoss: 0.006834\n",
      "Train Epoch: 5 [105600/332987 (32%)]\tLoss: 0.039823\n",
      "Train Epoch: 5 [108800/332987 (33%)]\tLoss: 0.059722\n",
      "Train Epoch: 5 [112000/332987 (34%)]\tLoss: 0.019217\n",
      "Train Epoch: 5 [115200/332987 (35%)]\tLoss: 0.003847\n",
      "Train Epoch: 5 [118400/332987 (36%)]\tLoss: 0.169784\n",
      "Train Epoch: 5 [121600/332987 (37%)]\tLoss: 0.017374\n",
      "Train Epoch: 5 [124800/332987 (37%)]\tLoss: 0.001222\n",
      "Train Epoch: 5 [128000/332987 (38%)]\tLoss: 0.001018\n",
      "Train Epoch: 5 [131200/332987 (39%)]\tLoss: 0.004804\n",
      "Train Epoch: 5 [134400/332987 (40%)]\tLoss: 0.008433\n",
      "Train Epoch: 5 [137600/332987 (41%)]\tLoss: 0.038579\n",
      "Train Epoch: 5 [140800/332987 (42%)]\tLoss: 0.063904\n",
      "Train Epoch: 5 [144000/332987 (43%)]\tLoss: 0.016457\n",
      "Train Epoch: 5 [147200/332987 (44%)]\tLoss: 0.063947\n",
      "Train Epoch: 5 [150400/332987 (45%)]\tLoss: 0.057101\n",
      "Train Epoch: 5 [153600/332987 (46%)]\tLoss: 0.090133\n",
      "Train Epoch: 5 [156800/332987 (47%)]\tLoss: 0.003051\n",
      "Train Epoch: 5 [160000/332987 (48%)]\tLoss: 0.044570\n",
      "Train Epoch: 5 [163200/332987 (49%)]\tLoss: 0.029673\n",
      "Train Epoch: 5 [166400/332987 (50%)]\tLoss: 0.005228\n",
      "Train Epoch: 5 [169600/332987 (51%)]\tLoss: 0.001306\n",
      "Train Epoch: 5 [172800/332987 (52%)]\tLoss: 0.006288\n",
      "Train Epoch: 5 [176000/332987 (53%)]\tLoss: 0.001882\n",
      "Train Epoch: 5 [179200/332987 (54%)]\tLoss: 0.000397\n",
      "Train Epoch: 5 [182400/332987 (55%)]\tLoss: 0.038375\n",
      "Train Epoch: 5 [185600/332987 (56%)]\tLoss: 0.052217\n",
      "Train Epoch: 5 [188800/332987 (57%)]\tLoss: 0.008142\n",
      "Train Epoch: 5 [192000/332987 (58%)]\tLoss: 0.025041\n",
      "Train Epoch: 5 [195200/332987 (59%)]\tLoss: 0.060565\n",
      "Train Epoch: 5 [198400/332987 (60%)]\tLoss: 0.029875\n",
      "Train Epoch: 5 [201600/332987 (61%)]\tLoss: 0.006830\n",
      "Train Epoch: 5 [204800/332987 (62%)]\tLoss: 0.008805\n",
      "Train Epoch: 5 [208000/332987 (62%)]\tLoss: 0.013612\n",
      "Train Epoch: 5 [211200/332987 (63%)]\tLoss: 0.014751\n",
      "Train Epoch: 5 [214400/332987 (64%)]\tLoss: 0.205293\n",
      "Train Epoch: 5 [217600/332987 (65%)]\tLoss: 0.069750\n",
      "Train Epoch: 5 [220800/332987 (66%)]\tLoss: 0.008466\n",
      "Train Epoch: 5 [224000/332987 (67%)]\tLoss: 0.005258\n",
      "Train Epoch: 5 [227200/332987 (68%)]\tLoss: 0.005314\n",
      "Train Epoch: 5 [230400/332987 (69%)]\tLoss: 0.139368\n",
      "Train Epoch: 5 [233600/332987 (70%)]\tLoss: 0.054757\n",
      "Train Epoch: 5 [236800/332987 (71%)]\tLoss: 0.121071\n",
      "Train Epoch: 5 [240000/332987 (72%)]\tLoss: 0.094012\n",
      "Train Epoch: 5 [243200/332987 (73%)]\tLoss: 0.005093\n",
      "Train Epoch: 5 [246400/332987 (74%)]\tLoss: 0.014558\n",
      "Train Epoch: 5 [249600/332987 (75%)]\tLoss: 0.008206\n",
      "Train Epoch: 5 [252800/332987 (76%)]\tLoss: 0.003688\n",
      "Train Epoch: 5 [256000/332987 (77%)]\tLoss: 0.004588\n",
      "Train Epoch: 5 [259200/332987 (78%)]\tLoss: 0.019008\n",
      "Train Epoch: 5 [262400/332987 (79%)]\tLoss: 0.032770\n",
      "Train Epoch: 5 [265600/332987 (80%)]\tLoss: 0.069291\n",
      "Train Epoch: 5 [268800/332987 (81%)]\tLoss: 0.005378\n",
      "Train Epoch: 5 [272000/332987 (82%)]\tLoss: 0.002460\n",
      "Train Epoch: 5 [275200/332987 (83%)]\tLoss: 0.013287\n",
      "Train Epoch: 5 [278400/332987 (84%)]\tLoss: 0.008954\n",
      "Train Epoch: 5 [281600/332987 (85%)]\tLoss: 0.001989\n",
      "Train Epoch: 5 [284800/332987 (86%)]\tLoss: 0.104031\n",
      "Train Epoch: 5 [288000/332987 (86%)]\tLoss: 0.043731\n",
      "Train Epoch: 5 [291200/332987 (87%)]\tLoss: 0.042503\n",
      "Train Epoch: 5 [294400/332987 (88%)]\tLoss: 0.017875\n",
      "Train Epoch: 5 [297600/332987 (89%)]\tLoss: 0.003901\n",
      "Train Epoch: 5 [300800/332987 (90%)]\tLoss: 0.078749\n",
      "Train Epoch: 5 [304000/332987 (91%)]\tLoss: 0.011346\n",
      "Train Epoch: 5 [307200/332987 (92%)]\tLoss: 0.087744\n",
      "Train Epoch: 5 [310400/332987 (93%)]\tLoss: 0.005107\n",
      "Train Epoch: 5 [313600/332987 (94%)]\tLoss: 0.015827\n",
      "Train Epoch: 5 [316800/332987 (95%)]\tLoss: 0.036455\n",
      "Train Epoch: 5 [320000/332987 (96%)]\tLoss: 0.009437\n",
      "Train Epoch: 5 [323200/332987 (97%)]\tLoss: 0.002292\n",
      "Train Epoch: 5 [326400/332987 (98%)]\tLoss: 0.017074\n",
      "Train Epoch: 5 [329600/332987 (99%)]\tLoss: 0.008194\n",
      "Train Epoch: 5 [332800/332987 (100%)]\tLoss: 0.064043\n",
      "Train Epoch: 6 [3200/332987 (1%)]\tLoss: 0.051684\n",
      "Train Epoch: 6 [6400/332987 (2%)]\tLoss: 0.000544\n",
      "Train Epoch: 6 [9600/332987 (3%)]\tLoss: 0.013835\n",
      "Train Epoch: 6 [12800/332987 (4%)]\tLoss: 0.022325\n",
      "Train Epoch: 6 [16000/332987 (5%)]\tLoss: 0.021035\n",
      "Train Epoch: 6 [19200/332987 (6%)]\tLoss: 0.003531\n",
      "Train Epoch: 6 [22400/332987 (7%)]\tLoss: 0.177074\n",
      "Train Epoch: 6 [25600/332987 (8%)]\tLoss: 0.033504\n",
      "Train Epoch: 6 [28800/332987 (9%)]\tLoss: 0.001505\n",
      "Train Epoch: 6 [32000/332987 (10%)]\tLoss: 0.007544\n",
      "Train Epoch: 6 [35200/332987 (11%)]\tLoss: 0.054975\n",
      "Train Epoch: 6 [38400/332987 (12%)]\tLoss: 0.010577\n",
      "Train Epoch: 6 [41600/332987 (12%)]\tLoss: 0.001805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [44800/332987 (13%)]\tLoss: 0.002870\n",
      "Train Epoch: 6 [48000/332987 (14%)]\tLoss: 0.137996\n",
      "Train Epoch: 6 [51200/332987 (15%)]\tLoss: 0.015037\n",
      "Train Epoch: 6 [54400/332987 (16%)]\tLoss: 0.002090\n",
      "Train Epoch: 6 [57600/332987 (17%)]\tLoss: 0.007295\n",
      "Train Epoch: 6 [60800/332987 (18%)]\tLoss: 0.008832\n",
      "Train Epoch: 6 [64000/332987 (19%)]\tLoss: 0.010191\n",
      "Train Epoch: 6 [67200/332987 (20%)]\tLoss: 0.008626\n",
      "Train Epoch: 6 [70400/332987 (21%)]\tLoss: 0.082952\n",
      "Train Epoch: 6 [73600/332987 (22%)]\tLoss: 0.010761\n",
      "Train Epoch: 6 [76800/332987 (23%)]\tLoss: 0.004870\n",
      "Train Epoch: 6 [80000/332987 (24%)]\tLoss: 0.039306\n",
      "Train Epoch: 6 [83200/332987 (25%)]\tLoss: 0.002738\n",
      "Train Epoch: 6 [86400/332987 (26%)]\tLoss: 0.072524\n",
      "Train Epoch: 6 [89600/332987 (27%)]\tLoss: 0.000947\n",
      "Train Epoch: 6 [92800/332987 (28%)]\tLoss: 0.007098\n",
      "Train Epoch: 6 [96000/332987 (29%)]\tLoss: 0.002874\n",
      "Train Epoch: 6 [99200/332987 (30%)]\tLoss: 0.001639\n",
      "Train Epoch: 6 [102400/332987 (31%)]\tLoss: 0.018022\n",
      "Train Epoch: 6 [105600/332987 (32%)]\tLoss: 0.006800\n",
      "Train Epoch: 6 [108800/332987 (33%)]\tLoss: 0.019357\n",
      "Train Epoch: 6 [112000/332987 (34%)]\tLoss: 0.004533\n",
      "Train Epoch: 6 [115200/332987 (35%)]\tLoss: 0.007810\n",
      "Train Epoch: 6 [118400/332987 (36%)]\tLoss: 0.001900\n",
      "Train Epoch: 6 [121600/332987 (37%)]\tLoss: 0.003375\n",
      "Train Epoch: 6 [124800/332987 (37%)]\tLoss: 0.009527\n",
      "Train Epoch: 6 [128000/332987 (38%)]\tLoss: 0.002409\n",
      "Train Epoch: 6 [131200/332987 (39%)]\tLoss: 0.015813\n",
      "Train Epoch: 6 [134400/332987 (40%)]\tLoss: 0.040158\n",
      "Train Epoch: 6 [137600/332987 (41%)]\tLoss: 0.004267\n",
      "Train Epoch: 6 [140800/332987 (42%)]\tLoss: 0.002931\n",
      "Train Epoch: 6 [144000/332987 (43%)]\tLoss: 0.000208\n",
      "Train Epoch: 6 [147200/332987 (44%)]\tLoss: 0.001507\n",
      "Train Epoch: 6 [150400/332987 (45%)]\tLoss: 0.002761\n",
      "Train Epoch: 6 [153600/332987 (46%)]\tLoss: 0.010670\n",
      "Train Epoch: 6 [156800/332987 (47%)]\tLoss: 0.005835\n",
      "Train Epoch: 6 [160000/332987 (48%)]\tLoss: 0.002826\n",
      "Train Epoch: 6 [163200/332987 (49%)]\tLoss: 0.002314\n",
      "Train Epoch: 6 [166400/332987 (50%)]\tLoss: 0.026645\n",
      "Train Epoch: 6 [169600/332987 (51%)]\tLoss: 0.000377\n",
      "Train Epoch: 6 [172800/332987 (52%)]\tLoss: 0.001472\n",
      "Train Epoch: 6 [176000/332987 (53%)]\tLoss: 0.003054\n",
      "Train Epoch: 6 [179200/332987 (54%)]\tLoss: 0.007505\n",
      "Train Epoch: 6 [182400/332987 (55%)]\tLoss: 0.012740\n",
      "Train Epoch: 6 [185600/332987 (56%)]\tLoss: 0.004106\n",
      "Train Epoch: 6 [188800/332987 (57%)]\tLoss: 0.050869\n",
      "Train Epoch: 6 [192000/332987 (58%)]\tLoss: 0.008270\n",
      "Train Epoch: 6 [195200/332987 (59%)]\tLoss: 0.005670\n",
      "Train Epoch: 6 [198400/332987 (60%)]\tLoss: 0.015910\n",
      "Train Epoch: 6 [201600/332987 (61%)]\tLoss: 0.003823\n",
      "Train Epoch: 6 [204800/332987 (62%)]\tLoss: 0.014179\n",
      "Train Epoch: 6 [208000/332987 (62%)]\tLoss: 0.008459\n",
      "Train Epoch: 6 [211200/332987 (63%)]\tLoss: 0.035497\n",
      "Train Epoch: 6 [214400/332987 (64%)]\tLoss: 0.001088\n",
      "Train Epoch: 6 [217600/332987 (65%)]\tLoss: 0.008348\n",
      "Train Epoch: 6 [220800/332987 (66%)]\tLoss: 0.006493\n",
      "Train Epoch: 6 [224000/332987 (67%)]\tLoss: 0.001296\n",
      "Train Epoch: 6 [227200/332987 (68%)]\tLoss: 0.001276\n",
      "Train Epoch: 6 [230400/332987 (69%)]\tLoss: 0.003175\n",
      "Train Epoch: 6 [233600/332987 (70%)]\tLoss: 0.057067\n",
      "Train Epoch: 6 [236800/332987 (71%)]\tLoss: 0.001629\n",
      "Train Epoch: 6 [240000/332987 (72%)]\tLoss: 0.003361\n",
      "Train Epoch: 6 [243200/332987 (73%)]\tLoss: 0.030133\n",
      "Train Epoch: 6 [246400/332987 (74%)]\tLoss: 0.064756\n",
      "Train Epoch: 6 [249600/332987 (75%)]\tLoss: 0.003373\n",
      "Train Epoch: 6 [252800/332987 (76%)]\tLoss: 0.006187\n",
      "Train Epoch: 6 [256000/332987 (77%)]\tLoss: 0.024273\n",
      "Train Epoch: 6 [259200/332987 (78%)]\tLoss: 0.001645\n",
      "Train Epoch: 6 [262400/332987 (79%)]\tLoss: 0.001795\n",
      "Train Epoch: 6 [265600/332987 (80%)]\tLoss: 0.052594\n",
      "Train Epoch: 6 [268800/332987 (81%)]\tLoss: 0.013456\n",
      "Train Epoch: 6 [272000/332987 (82%)]\tLoss: 0.054356\n",
      "Train Epoch: 6 [275200/332987 (83%)]\tLoss: 0.030560\n",
      "Train Epoch: 6 [278400/332987 (84%)]\tLoss: 0.007296\n",
      "Train Epoch: 6 [281600/332987 (85%)]\tLoss: 0.031909\n",
      "Train Epoch: 6 [284800/332987 (86%)]\tLoss: 0.219218\n",
      "Train Epoch: 6 [288000/332987 (86%)]\tLoss: 0.002454\n",
      "Train Epoch: 6 [291200/332987 (87%)]\tLoss: 0.000384\n",
      "Train Epoch: 6 [294400/332987 (88%)]\tLoss: 0.072127\n",
      "Train Epoch: 6 [297600/332987 (89%)]\tLoss: 0.013375\n",
      "Train Epoch: 6 [300800/332987 (90%)]\tLoss: 0.033808\n",
      "Train Epoch: 6 [304000/332987 (91%)]\tLoss: 0.024430\n",
      "Train Epoch: 6 [307200/332987 (92%)]\tLoss: 0.021697\n",
      "Train Epoch: 6 [310400/332987 (93%)]\tLoss: 0.012158\n",
      "Train Epoch: 6 [313600/332987 (94%)]\tLoss: 0.006134\n",
      "Train Epoch: 6 [316800/332987 (95%)]\tLoss: 0.003922\n",
      "Train Epoch: 6 [320000/332987 (96%)]\tLoss: 0.122993\n",
      "Train Epoch: 6 [323200/332987 (97%)]\tLoss: 0.001956\n",
      "Train Epoch: 6 [326400/332987 (98%)]\tLoss: 0.058393\n",
      "Train Epoch: 6 [329600/332987 (99%)]\tLoss: 0.021491\n",
      "Train Epoch: 6 [332800/332987 (100%)]\tLoss: 0.011696\n",
      "Train Epoch: 7 [3200/332987 (1%)]\tLoss: 0.001666\n",
      "Train Epoch: 7 [6400/332987 (2%)]\tLoss: 0.022079\n",
      "Train Epoch: 7 [9600/332987 (3%)]\tLoss: 0.021003\n",
      "Train Epoch: 7 [12800/332987 (4%)]\tLoss: 0.005324\n",
      "Train Epoch: 7 [16000/332987 (5%)]\tLoss: 0.053608\n",
      "Train Epoch: 7 [19200/332987 (6%)]\tLoss: 0.007434\n",
      "Train Epoch: 7 [22400/332987 (7%)]\tLoss: 0.005130\n",
      "Train Epoch: 7 [25600/332987 (8%)]\tLoss: 0.004298\n",
      "Train Epoch: 7 [28800/332987 (9%)]\tLoss: 0.001270\n",
      "Train Epoch: 7 [32000/332987 (10%)]\tLoss: 0.004147\n",
      "Train Epoch: 7 [35200/332987 (11%)]\tLoss: 0.084148\n",
      "Train Epoch: 7 [38400/332987 (12%)]\tLoss: 0.003510\n",
      "Train Epoch: 7 [41600/332987 (12%)]\tLoss: 0.053224\n",
      "Train Epoch: 7 [44800/332987 (13%)]\tLoss: 0.000916\n",
      "Train Epoch: 7 [48000/332987 (14%)]\tLoss: 0.032512\n",
      "Train Epoch: 7 [51200/332987 (15%)]\tLoss: 0.012323\n",
      "Train Epoch: 7 [54400/332987 (16%)]\tLoss: 0.010784\n",
      "Train Epoch: 7 [57600/332987 (17%)]\tLoss: 0.017979\n",
      "Train Epoch: 7 [60800/332987 (18%)]\tLoss: 0.002905\n",
      "Train Epoch: 7 [64000/332987 (19%)]\tLoss: 0.034833\n",
      "Train Epoch: 7 [67200/332987 (20%)]\tLoss: 0.002651\n",
      "Train Epoch: 7 [70400/332987 (21%)]\tLoss: 0.001418\n",
      "Train Epoch: 7 [73600/332987 (22%)]\tLoss: 0.001994\n",
      "Train Epoch: 7 [76800/332987 (23%)]\tLoss: 0.004441\n",
      "Train Epoch: 7 [80000/332987 (24%)]\tLoss: 0.004538\n",
      "Train Epoch: 7 [83200/332987 (25%)]\tLoss: 0.001328\n",
      "Train Epoch: 7 [86400/332987 (26%)]\tLoss: 0.092496\n",
      "Train Epoch: 7 [89600/332987 (27%)]\tLoss: 0.027728\n",
      "Train Epoch: 7 [92800/332987 (28%)]\tLoss: 0.009330\n",
      "Train Epoch: 7 [96000/332987 (29%)]\tLoss: 0.007748\n",
      "Train Epoch: 7 [99200/332987 (30%)]\tLoss: 0.057506\n",
      "Train Epoch: 7 [102400/332987 (31%)]\tLoss: 0.001991\n",
      "Train Epoch: 7 [105600/332987 (32%)]\tLoss: 0.008692\n",
      "Train Epoch: 7 [108800/332987 (33%)]\tLoss: 0.034455\n",
      "Train Epoch: 7 [112000/332987 (34%)]\tLoss: 0.050141\n",
      "Train Epoch: 7 [115200/332987 (35%)]\tLoss: 0.006590\n",
      "Train Epoch: 7 [118400/332987 (36%)]\tLoss: 0.012179\n",
      "Train Epoch: 7 [121600/332987 (37%)]\tLoss: 0.064923\n",
      "Train Epoch: 7 [124800/332987 (37%)]\tLoss: 0.012480\n",
      "Train Epoch: 7 [128000/332987 (38%)]\tLoss: 0.005589\n",
      "Train Epoch: 7 [131200/332987 (39%)]\tLoss: 0.001502\n",
      "Train Epoch: 7 [134400/332987 (40%)]\tLoss: 0.005924\n",
      "Train Epoch: 7 [137600/332987 (41%)]\tLoss: 0.001625\n",
      "Train Epoch: 7 [140800/332987 (42%)]\tLoss: 0.006834\n",
      "Train Epoch: 7 [144000/332987 (43%)]\tLoss: 0.002621\n",
      "Train Epoch: 7 [147200/332987 (44%)]\tLoss: 0.004293\n",
      "Train Epoch: 7 [150400/332987 (45%)]\tLoss: 0.000252\n",
      "Train Epoch: 7 [153600/332987 (46%)]\tLoss: 0.003269\n",
      "Train Epoch: 7 [156800/332987 (47%)]\tLoss: 0.008081\n",
      "Train Epoch: 7 [160000/332987 (48%)]\tLoss: 0.031684\n",
      "Train Epoch: 7 [163200/332987 (49%)]\tLoss: 0.010997\n",
      "Train Epoch: 7 [166400/332987 (50%)]\tLoss: 0.002697\n",
      "Train Epoch: 7 [169600/332987 (51%)]\tLoss: 0.007131\n",
      "Train Epoch: 7 [172800/332987 (52%)]\tLoss: 0.022758\n",
      "Train Epoch: 7 [176000/332987 (53%)]\tLoss: 0.034564\n",
      "Train Epoch: 7 [179200/332987 (54%)]\tLoss: 0.001782\n",
      "Train Epoch: 7 [182400/332987 (55%)]\tLoss: 0.002807\n",
      "Train Epoch: 7 [185600/332987 (56%)]\tLoss: 0.001411\n",
      "Train Epoch: 7 [188800/332987 (57%)]\tLoss: 0.032662\n",
      "Train Epoch: 7 [192000/332987 (58%)]\tLoss: 0.002372\n",
      "Train Epoch: 7 [195200/332987 (59%)]\tLoss: 0.002154\n",
      "Train Epoch: 7 [198400/332987 (60%)]\tLoss: 0.023442\n",
      "Train Epoch: 7 [201600/332987 (61%)]\tLoss: 0.005695\n",
      "Train Epoch: 7 [204800/332987 (62%)]\tLoss: 0.002762\n",
      "Train Epoch: 7 [208000/332987 (62%)]\tLoss: 0.001345\n",
      "Train Epoch: 7 [211200/332987 (63%)]\tLoss: 0.000688\n",
      "Train Epoch: 7 [214400/332987 (64%)]\tLoss: 0.001339\n",
      "Train Epoch: 7 [217600/332987 (65%)]\tLoss: 0.005725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [220800/332987 (66%)]\tLoss: 0.002386\n",
      "Train Epoch: 7 [224000/332987 (67%)]\tLoss: 0.015195\n",
      "Train Epoch: 7 [227200/332987 (68%)]\tLoss: 0.014855\n",
      "Train Epoch: 7 [230400/332987 (69%)]\tLoss: 0.001934\n",
      "Train Epoch: 7 [233600/332987 (70%)]\tLoss: 0.042575\n",
      "Train Epoch: 7 [236800/332987 (71%)]\tLoss: 0.007519\n",
      "Train Epoch: 7 [240000/332987 (72%)]\tLoss: 0.009602\n",
      "Train Epoch: 7 [243200/332987 (73%)]\tLoss: 0.001990\n",
      "Train Epoch: 7 [246400/332987 (74%)]\tLoss: 0.051574\n",
      "Train Epoch: 7 [249600/332987 (75%)]\tLoss: 0.001826\n",
      "Train Epoch: 7 [252800/332987 (76%)]\tLoss: 0.002280\n",
      "Train Epoch: 7 [256000/332987 (77%)]\tLoss: 0.002018\n",
      "Train Epoch: 7 [259200/332987 (78%)]\tLoss: 0.058622\n",
      "Train Epoch: 7 [262400/332987 (79%)]\tLoss: 0.021916\n",
      "Train Epoch: 7 [265600/332987 (80%)]\tLoss: 0.003297\n",
      "Train Epoch: 7 [268800/332987 (81%)]\tLoss: 0.006752\n",
      "Train Epoch: 7 [272000/332987 (82%)]\tLoss: 0.014814\n",
      "Train Epoch: 7 [275200/332987 (83%)]\tLoss: 0.003632\n",
      "Train Epoch: 7 [278400/332987 (84%)]\tLoss: 0.000583\n",
      "Train Epoch: 7 [281600/332987 (85%)]\tLoss: 0.000426\n",
      "Train Epoch: 7 [284800/332987 (86%)]\tLoss: 0.012000\n",
      "Train Epoch: 7 [288000/332987 (86%)]\tLoss: 0.000777\n",
      "Train Epoch: 7 [291200/332987 (87%)]\tLoss: 0.011201\n",
      "Train Epoch: 7 [294400/332987 (88%)]\tLoss: 0.005134\n",
      "Train Epoch: 7 [297600/332987 (89%)]\tLoss: 0.003091\n",
      "Train Epoch: 7 [300800/332987 (90%)]\tLoss: 0.001380\n",
      "Train Epoch: 7 [304000/332987 (91%)]\tLoss: 0.000143\n",
      "Train Epoch: 7 [307200/332987 (92%)]\tLoss: 0.001343\n",
      "Train Epoch: 7 [310400/332987 (93%)]\tLoss: 0.016977\n",
      "Train Epoch: 7 [313600/332987 (94%)]\tLoss: 0.003229\n",
      "Train Epoch: 7 [316800/332987 (95%)]\tLoss: 0.006817\n",
      "Train Epoch: 7 [320000/332987 (96%)]\tLoss: 0.001133\n",
      "Train Epoch: 7 [323200/332987 (97%)]\tLoss: 0.001142\n",
      "Train Epoch: 7 [326400/332987 (98%)]\tLoss: 0.014759\n",
      "Train Epoch: 7 [329600/332987 (99%)]\tLoss: 0.164620\n",
      "Train Epoch: 7 [332800/332987 (100%)]\tLoss: 0.021270\n",
      "Train Epoch: 8 [3200/332987 (1%)]\tLoss: 0.000259\n",
      "Train Epoch: 8 [6400/332987 (2%)]\tLoss: 0.009146\n",
      "Train Epoch: 8 [9600/332987 (3%)]\tLoss: 0.002125\n",
      "Train Epoch: 8 [12800/332987 (4%)]\tLoss: 0.027275\n",
      "Train Epoch: 8 [16000/332987 (5%)]\tLoss: 0.001327\n",
      "Train Epoch: 8 [19200/332987 (6%)]\tLoss: 0.008347\n",
      "Train Epoch: 8 [22400/332987 (7%)]\tLoss: 0.000309\n",
      "Train Epoch: 8 [25600/332987 (8%)]\tLoss: 0.001116\n",
      "Train Epoch: 8 [28800/332987 (9%)]\tLoss: 0.002470\n",
      "Train Epoch: 8 [32000/332987 (10%)]\tLoss: 0.009565\n",
      "Train Epoch: 8 [35200/332987 (11%)]\tLoss: 0.000204\n",
      "Train Epoch: 8 [38400/332987 (12%)]\tLoss: 0.049506\n",
      "Train Epoch: 8 [41600/332987 (12%)]\tLoss: 0.001691\n",
      "Train Epoch: 8 [44800/332987 (13%)]\tLoss: 0.049438\n",
      "Train Epoch: 8 [48000/332987 (14%)]\tLoss: 0.062363\n",
      "Train Epoch: 8 [51200/332987 (15%)]\tLoss: 0.000817\n",
      "Train Epoch: 8 [54400/332987 (16%)]\tLoss: 0.004090\n",
      "Train Epoch: 8 [57600/332987 (17%)]\tLoss: 0.043722\n",
      "Train Epoch: 8 [60800/332987 (18%)]\tLoss: 0.013962\n",
      "Train Epoch: 8 [64000/332987 (19%)]\tLoss: 0.003480\n",
      "Train Epoch: 8 [67200/332987 (20%)]\tLoss: 0.000489\n",
      "Train Epoch: 8 [70400/332987 (21%)]\tLoss: 0.015089\n",
      "Train Epoch: 8 [73600/332987 (22%)]\tLoss: 0.002705\n",
      "Train Epoch: 8 [76800/332987 (23%)]\tLoss: 0.006607\n",
      "Train Epoch: 8 [80000/332987 (24%)]\tLoss: 0.000945\n",
      "Train Epoch: 8 [83200/332987 (25%)]\tLoss: 0.000851\n",
      "Train Epoch: 8 [86400/332987 (26%)]\tLoss: 0.013712\n",
      "Train Epoch: 8 [89600/332987 (27%)]\tLoss: 0.000855\n",
      "Train Epoch: 8 [92800/332987 (28%)]\tLoss: 0.003385\n",
      "Train Epoch: 8 [96000/332987 (29%)]\tLoss: 0.015320\n",
      "Train Epoch: 8 [99200/332987 (30%)]\tLoss: 0.000455\n",
      "Train Epoch: 8 [102400/332987 (31%)]\tLoss: 0.001155\n",
      "Train Epoch: 8 [105600/332987 (32%)]\tLoss: 0.009569\n",
      "Train Epoch: 8 [108800/332987 (33%)]\tLoss: 0.034538\n",
      "Train Epoch: 8 [112000/332987 (34%)]\tLoss: 0.000202\n",
      "Train Epoch: 8 [115200/332987 (35%)]\tLoss: 0.000832\n",
      "Train Epoch: 8 [118400/332987 (36%)]\tLoss: 0.007214\n",
      "Train Epoch: 8 [121600/332987 (37%)]\tLoss: 0.036351\n",
      "Train Epoch: 8 [124800/332987 (37%)]\tLoss: 0.004104\n",
      "Train Epoch: 8 [128000/332987 (38%)]\tLoss: 0.001130\n",
      "Train Epoch: 8 [131200/332987 (39%)]\tLoss: 0.078736\n",
      "Train Epoch: 8 [134400/332987 (40%)]\tLoss: 0.001352\n",
      "Train Epoch: 8 [137600/332987 (41%)]\tLoss: 0.005819\n",
      "Train Epoch: 8 [140800/332987 (42%)]\tLoss: 0.003243\n",
      "Train Epoch: 8 [144000/332987 (43%)]\tLoss: 0.003112\n",
      "Train Epoch: 8 [147200/332987 (44%)]\tLoss: 0.009800\n",
      "Train Epoch: 8 [150400/332987 (45%)]\tLoss: 0.008504\n",
      "Train Epoch: 8 [153600/332987 (46%)]\tLoss: 0.006950\n",
      "Train Epoch: 8 [156800/332987 (47%)]\tLoss: 0.001426\n",
      "Train Epoch: 8 [160000/332987 (48%)]\tLoss: 0.002538\n",
      "Train Epoch: 8 [163200/332987 (49%)]\tLoss: 0.001768\n",
      "Train Epoch: 8 [166400/332987 (50%)]\tLoss: 0.000860\n",
      "Train Epoch: 8 [169600/332987 (51%)]\tLoss: 0.010114\n",
      "Train Epoch: 8 [172800/332987 (52%)]\tLoss: 0.001848\n",
      "Train Epoch: 8 [176000/332987 (53%)]\tLoss: 0.000719\n",
      "Train Epoch: 8 [179200/332987 (54%)]\tLoss: 0.011336\n",
      "Train Epoch: 8 [182400/332987 (55%)]\tLoss: 0.013036\n",
      "Train Epoch: 8 [185600/332987 (56%)]\tLoss: 0.003062\n",
      "Train Epoch: 8 [188800/332987 (57%)]\tLoss: 0.004051\n",
      "Train Epoch: 8 [192000/332987 (58%)]\tLoss: 0.000406\n",
      "Train Epoch: 8 [195200/332987 (59%)]\tLoss: 0.012717\n",
      "Train Epoch: 8 [198400/332987 (60%)]\tLoss: 0.017138\n",
      "Train Epoch: 8 [201600/332987 (61%)]\tLoss: 0.000686\n",
      "Train Epoch: 8 [204800/332987 (62%)]\tLoss: 0.005302\n",
      "Train Epoch: 8 [208000/332987 (62%)]\tLoss: 0.001799\n",
      "Train Epoch: 8 [211200/332987 (63%)]\tLoss: 0.000360\n",
      "Train Epoch: 8 [214400/332987 (64%)]\tLoss: 0.003843\n",
      "Train Epoch: 8 [217600/332987 (65%)]\tLoss: 0.006214\n",
      "Train Epoch: 8 [220800/332987 (66%)]\tLoss: 0.004422\n",
      "Train Epoch: 8 [224000/332987 (67%)]\tLoss: 0.003090\n",
      "Train Epoch: 8 [227200/332987 (68%)]\tLoss: 0.001156\n",
      "Train Epoch: 8 [230400/332987 (69%)]\tLoss: 0.006286\n",
      "Train Epoch: 8 [233600/332987 (70%)]\tLoss: 0.002374\n",
      "Train Epoch: 8 [236800/332987 (71%)]\tLoss: 0.004134\n",
      "Train Epoch: 8 [240000/332987 (72%)]\tLoss: 0.025590\n",
      "Train Epoch: 8 [243200/332987 (73%)]\tLoss: 0.076778\n",
      "Train Epoch: 8 [246400/332987 (74%)]\tLoss: 0.000531\n",
      "Train Epoch: 8 [249600/332987 (75%)]\tLoss: 0.000908\n",
      "Train Epoch: 8 [252800/332987 (76%)]\tLoss: 0.001867\n",
      "Train Epoch: 8 [256000/332987 (77%)]\tLoss: 0.001816\n",
      "Train Epoch: 8 [259200/332987 (78%)]\tLoss: 0.000628\n",
      "Train Epoch: 8 [262400/332987 (79%)]\tLoss: 0.006311\n",
      "Train Epoch: 8 [265600/332987 (80%)]\tLoss: 0.000227\n",
      "Train Epoch: 8 [268800/332987 (81%)]\tLoss: 0.004675\n",
      "Train Epoch: 8 [272000/332987 (82%)]\tLoss: 0.000934\n",
      "Train Epoch: 8 [275200/332987 (83%)]\tLoss: 0.005326\n",
      "Train Epoch: 8 [278400/332987 (84%)]\tLoss: 0.004416\n",
      "Train Epoch: 8 [281600/332987 (85%)]\tLoss: 0.004073\n",
      "Train Epoch: 8 [284800/332987 (86%)]\tLoss: 0.081789\n",
      "Train Epoch: 8 [288000/332987 (86%)]\tLoss: 0.001621\n",
      "Train Epoch: 8 [291200/332987 (87%)]\tLoss: 0.000552\n",
      "Train Epoch: 8 [294400/332987 (88%)]\tLoss: 0.001552\n",
      "Train Epoch: 8 [297600/332987 (89%)]\tLoss: 0.034381\n",
      "Train Epoch: 8 [300800/332987 (90%)]\tLoss: 0.007894\n",
      "Train Epoch: 8 [304000/332987 (91%)]\tLoss: 0.008217\n",
      "Train Epoch: 8 [307200/332987 (92%)]\tLoss: 0.012713\n",
      "Train Epoch: 8 [310400/332987 (93%)]\tLoss: 0.000798\n",
      "Train Epoch: 8 [313600/332987 (94%)]\tLoss: 0.015917\n",
      "Train Epoch: 8 [316800/332987 (95%)]\tLoss: 0.001259\n",
      "Train Epoch: 8 [320000/332987 (96%)]\tLoss: 0.001493\n",
      "Train Epoch: 8 [323200/332987 (97%)]\tLoss: 0.001658\n",
      "Train Epoch: 8 [326400/332987 (98%)]\tLoss: 0.003210\n",
      "Train Epoch: 8 [329600/332987 (99%)]\tLoss: 0.000693\n",
      "Train Epoch: 8 [332800/332987 (100%)]\tLoss: 0.015757\n",
      "Train Epoch: 9 [3200/332987 (1%)]\tLoss: 0.001816\n",
      "Train Epoch: 9 [6400/332987 (2%)]\tLoss: 0.000712\n",
      "Train Epoch: 9 [9600/332987 (3%)]\tLoss: 0.008448\n",
      "Train Epoch: 9 [12800/332987 (4%)]\tLoss: 0.000550\n",
      "Train Epoch: 9 [16000/332987 (5%)]\tLoss: 0.012946\n",
      "Train Epoch: 9 [19200/332987 (6%)]\tLoss: 0.000285\n",
      "Train Epoch: 9 [22400/332987 (7%)]\tLoss: 0.003314\n",
      "Train Epoch: 9 [25600/332987 (8%)]\tLoss: 0.013674\n",
      "Train Epoch: 9 [28800/332987 (9%)]\tLoss: 0.001266\n",
      "Train Epoch: 9 [32000/332987 (10%)]\tLoss: 0.014772\n",
      "Train Epoch: 9 [35200/332987 (11%)]\tLoss: 0.003062\n",
      "Train Epoch: 9 [38400/332987 (12%)]\tLoss: 0.009403\n",
      "Train Epoch: 9 [41600/332987 (12%)]\tLoss: 0.000510\n",
      "Train Epoch: 9 [44800/332987 (13%)]\tLoss: 0.006642\n",
      "Train Epoch: 9 [48000/332987 (14%)]\tLoss: 0.001148\n",
      "Train Epoch: 9 [51200/332987 (15%)]\tLoss: 0.004031\n",
      "Train Epoch: 9 [54400/332987 (16%)]\tLoss: 0.005570\n",
      "Train Epoch: 9 [57600/332987 (17%)]\tLoss: 0.002470\n",
      "Train Epoch: 9 [60800/332987 (18%)]\tLoss: 0.006334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [64000/332987 (19%)]\tLoss: 0.000358\n",
      "Train Epoch: 9 [67200/332987 (20%)]\tLoss: 0.013037\n",
      "Train Epoch: 9 [70400/332987 (21%)]\tLoss: 0.003248\n",
      "Train Epoch: 9 [73600/332987 (22%)]\tLoss: 0.000427\n",
      "Train Epoch: 9 [76800/332987 (23%)]\tLoss: 0.003115\n",
      "Train Epoch: 9 [80000/332987 (24%)]\tLoss: 0.014842\n",
      "Train Epoch: 9 [83200/332987 (25%)]\tLoss: 0.001740\n",
      "Train Epoch: 9 [86400/332987 (26%)]\tLoss: 0.001067\n",
      "Train Epoch: 9 [89600/332987 (27%)]\tLoss: 0.005257\n",
      "Train Epoch: 9 [92800/332987 (28%)]\tLoss: 0.003975\n",
      "Train Epoch: 9 [96000/332987 (29%)]\tLoss: 0.057031\n",
      "Train Epoch: 9 [99200/332987 (30%)]\tLoss: 0.006414\n",
      "Train Epoch: 9 [102400/332987 (31%)]\tLoss: 0.003508\n",
      "Train Epoch: 9 [105600/332987 (32%)]\tLoss: 0.000756\n",
      "Train Epoch: 9 [108800/332987 (33%)]\tLoss: 0.000824\n",
      "Train Epoch: 9 [112000/332987 (34%)]\tLoss: 0.006167\n",
      "Train Epoch: 9 [115200/332987 (35%)]\tLoss: 0.001879\n",
      "Train Epoch: 9 [118400/332987 (36%)]\tLoss: 0.000953\n",
      "Train Epoch: 9 [121600/332987 (37%)]\tLoss: 0.028236\n",
      "Train Epoch: 9 [124800/332987 (37%)]\tLoss: 0.000668\n",
      "Train Epoch: 9 [128000/332987 (38%)]\tLoss: 0.011696\n",
      "Train Epoch: 9 [131200/332987 (39%)]\tLoss: 0.006653\n",
      "Train Epoch: 9 [134400/332987 (40%)]\tLoss: 0.000872\n",
      "Train Epoch: 9 [137600/332987 (41%)]\tLoss: 0.000779\n",
      "Train Epoch: 9 [140800/332987 (42%)]\tLoss: 0.000218\n",
      "Train Epoch: 9 [144000/332987 (43%)]\tLoss: 0.008547\n",
      "Train Epoch: 9 [147200/332987 (44%)]\tLoss: 0.011235\n",
      "Train Epoch: 9 [150400/332987 (45%)]\tLoss: 0.000182\n",
      "Train Epoch: 9 [153600/332987 (46%)]\tLoss: 0.013397\n",
      "Train Epoch: 9 [156800/332987 (47%)]\tLoss: 0.000874\n",
      "Train Epoch: 9 [160000/332987 (48%)]\tLoss: 0.014167\n",
      "Train Epoch: 9 [163200/332987 (49%)]\tLoss: 0.005236\n",
      "Train Epoch: 9 [166400/332987 (50%)]\tLoss: 0.001343\n",
      "Train Epoch: 9 [169600/332987 (51%)]\tLoss: 0.126739\n",
      "Train Epoch: 9 [172800/332987 (52%)]\tLoss: 0.007889\n",
      "Train Epoch: 9 [176000/332987 (53%)]\tLoss: 0.002064\n",
      "Train Epoch: 9 [179200/332987 (54%)]\tLoss: 0.002404\n",
      "Train Epoch: 9 [182400/332987 (55%)]\tLoss: 0.005033\n",
      "Train Epoch: 9 [185600/332987 (56%)]\tLoss: 0.000814\n",
      "Train Epoch: 9 [188800/332987 (57%)]\tLoss: 0.014145\n",
      "Train Epoch: 9 [192000/332987 (58%)]\tLoss: 0.001221\n",
      "Train Epoch: 9 [195200/332987 (59%)]\tLoss: 0.004774\n",
      "Train Epoch: 9 [198400/332987 (60%)]\tLoss: 0.000414\n",
      "Train Epoch: 9 [201600/332987 (61%)]\tLoss: 0.002686\n",
      "Train Epoch: 9 [204800/332987 (62%)]\tLoss: 0.001323\n",
      "Train Epoch: 9 [208000/332987 (62%)]\tLoss: 0.021692\n",
      "Train Epoch: 9 [211200/332987 (63%)]\tLoss: 0.004939\n",
      "Train Epoch: 9 [214400/332987 (64%)]\tLoss: 0.000422\n",
      "Train Epoch: 9 [217600/332987 (65%)]\tLoss: 0.001001\n",
      "Train Epoch: 9 [220800/332987 (66%)]\tLoss: 0.002856\n",
      "Train Epoch: 9 [224000/332987 (67%)]\tLoss: 0.011252\n",
      "Train Epoch: 9 [227200/332987 (68%)]\tLoss: 0.004851\n",
      "Train Epoch: 9 [230400/332987 (69%)]\tLoss: 0.000902\n",
      "Train Epoch: 9 [233600/332987 (70%)]\tLoss: 0.004741\n",
      "Train Epoch: 9 [236800/332987 (71%)]\tLoss: 0.000658\n",
      "Train Epoch: 9 [240000/332987 (72%)]\tLoss: 0.000614\n",
      "Train Epoch: 9 [243200/332987 (73%)]\tLoss: 0.000220\n",
      "Train Epoch: 9 [246400/332987 (74%)]\tLoss: 0.077633\n",
      "Train Epoch: 9 [249600/332987 (75%)]\tLoss: 0.001727\n",
      "Train Epoch: 9 [252800/332987 (76%)]\tLoss: 0.011281\n",
      "Train Epoch: 9 [256000/332987 (77%)]\tLoss: 0.003210\n",
      "Train Epoch: 9 [259200/332987 (78%)]\tLoss: 0.000649\n",
      "Train Epoch: 9 [262400/332987 (79%)]\tLoss: 0.001066\n",
      "Train Epoch: 9 [265600/332987 (80%)]\tLoss: 0.005222\n",
      "Train Epoch: 9 [268800/332987 (81%)]\tLoss: 0.016158\n",
      "Train Epoch: 9 [272000/332987 (82%)]\tLoss: 0.002342\n",
      "Train Epoch: 9 [275200/332987 (83%)]\tLoss: 0.019447\n",
      "Train Epoch: 9 [278400/332987 (84%)]\tLoss: 0.000922\n",
      "Train Epoch: 9 [281600/332987 (85%)]\tLoss: 0.129268\n",
      "Train Epoch: 9 [284800/332987 (86%)]\tLoss: 0.000529\n",
      "Train Epoch: 9 [288000/332987 (86%)]\tLoss: 0.003187\n",
      "Train Epoch: 9 [291200/332987 (87%)]\tLoss: 0.000191\n",
      "Train Epoch: 9 [294400/332987 (88%)]\tLoss: 0.034175\n",
      "Train Epoch: 9 [297600/332987 (89%)]\tLoss: 0.002533\n",
      "Train Epoch: 9 [300800/332987 (90%)]\tLoss: 0.000254\n",
      "Train Epoch: 9 [304000/332987 (91%)]\tLoss: 0.009887\n",
      "Train Epoch: 9 [307200/332987 (92%)]\tLoss: 0.000599\n",
      "Train Epoch: 9 [310400/332987 (93%)]\tLoss: 0.021836\n",
      "Train Epoch: 9 [313600/332987 (94%)]\tLoss: 0.013345\n",
      "Train Epoch: 9 [316800/332987 (95%)]\tLoss: 0.000190\n",
      "Train Epoch: 9 [320000/332987 (96%)]\tLoss: 0.000721\n",
      "Train Epoch: 9 [323200/332987 (97%)]\tLoss: 0.002683\n",
      "Train Epoch: 9 [326400/332987 (98%)]\tLoss: 0.000379\n",
      "Train Epoch: 9 [329600/332987 (99%)]\tLoss: 0.002627\n",
      "Train Epoch: 9 [332800/332987 (100%)]\tLoss: 0.013464\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    lr_scheduler.step()\n",
    "    train(epoch)\n",
    "#    evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_np\n",
    "del train_y_np\n",
    "#del test_x_np\n",
    "#del test_y_np\n",
    "del train_loader\n",
    "#del test_loader\n",
    "del train_dataset\n",
    "#del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "14476d7c3d1a803d26b9631c340152777557698b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83247/83247 [00:17<00:00, 4777.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83247/83247 [00:00<00:00, 1372556.30it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = np.load('test.npy')\n",
    "pred = np.array([resize(img) for img in tqdm(pred)])\n",
    "pred = np.array([i[:, :, np.newaxis] for i in tqdm(pred)])\n",
    "pred_dataset = Hieroglyph_data(pred, train = False)\n",
    "pred_loader = torch.utils.data.DataLoader(pred_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "aca139420dc0c04f4062076f1d56facf7721a98d"
   },
   "outputs": [],
   "source": [
    "def prediciton(data_loader):\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, images in enumerate(data_loader):\n",
    "            images = Variable(images.view(-1, 1, h, w))\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            pred = outputs.cpu().data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "        \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "18d93e79f4c49227ef3179a642fdf10ee927acc4"
   },
   "outputs": [],
   "source": [
    "pred_y = prediciton(pred_loader)\n",
    "pred_y = pred_y.cpu().numpy().flatten()\n",
    "\n",
    "dct = {v:k for k,v in dct.items()}\n",
    "\n",
    "pred_y = [dct[code] for code in pred_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "46da802ed7e355968bfbc3fa760add2c238ea8f4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.Series(pred_y, name=\"Category\", dtype = 'object')\n",
    "\n",
    "submission = pd.concat([pd.Series(range(1,83248), name = \"Id\", dtype='object'), results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"dpn92_64_64_no_validation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3c7ba3c52ce70e7ca473ee77a22130b16975382"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
